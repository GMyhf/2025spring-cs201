# ç¥ç»ç½‘ç»œä¸­çš„åå‘ä¼ æ’­

Updated 2025-11-27 15:00 GMT+8*  
*Compiled by Hongfei Yan (2025 Spring)*   



Backpropagation in Neural Network

https://www.geeksforgeeks.org/machine-learning/backpropagation-in-neural-network/



åå‘ä¼ æ’­ï¼ˆBack Propagationï¼‰ï¼Œåˆç§°ä¸ºâ€œè¯¯å·®çš„åå‘ä¼ æ’­â€ï¼Œæ˜¯ä¸€ç§ç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œçš„æ–¹æ³•ã€‚å…¶ç›®æ ‡æ˜¯é€šè¿‡è°ƒæ•´ç½‘ç»œä¸­çš„æƒé‡ï¼ˆweightsï¼‰å’Œåç½®ï¼ˆbiasesï¼‰ï¼Œæ¥å‡å°æ¨¡å‹é¢„æµ‹è¾“å‡ºä¸å®é™…è¾“å‡ºä¹‹é—´çš„å·®å¼‚ã€‚

å®ƒé€šè¿‡è¿­ä»£æ–¹å¼æ›´æ–°æƒé‡å’Œåç½®ï¼Œä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼ˆcost functionï¼‰ã€‚åœ¨æ¯ä¸€ä¸ªè®­ç»ƒå‘¨æœŸï¼ˆepochï¼‰ä¸­ï¼Œæ¨¡å‹ä¼šæ ¹æ®è¯¯å·®æ¢¯åº¦ï¼ˆerror gradientï¼‰æ›´æ–°å‚æ•°ï¼Œå¸¸ç”¨çš„ä¼˜åŒ–ç®—æ³•åŒ…æ‹¬æ¢¯åº¦ä¸‹é™ï¼ˆGradient Descentï¼‰æˆ–éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ã€‚è¯¥ç®—æ³•ä½¿ç”¨å¾®ç§¯åˆ†ä¸­çš„<mark>é“¾å¼æ³•åˆ™</mark>æ¥è®¡ç®—æ¢¯åº¦ï¼Œä»è€Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ç©¿è¶Šå¤æ‚çš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œä¼˜åŒ–æŸå¤±å‡½æ•°ã€‚

> Back Propagation is also known as "Backward Propagation of Errors" is a method used to train neural network . Its goal is to reduce the difference between the modelâ€™s predicted output and the actual output by adjusting the weights and biases in the network.
>
> It works iteratively to adjust weights and bias to minimize the cost function. In each epoch the model adapts these parameters by reducing loss by following the error gradient. It often uses optimization algorithms like **gradient descent** or **stochastic gradient descent**. The algorithm computes the gradient using the chain rule from calculus allowing it to effectively navigate complex layers in the neural network to minimize the cost function.

<img src="https://media.geeksforgeeks.org/wp-content/uploads/20250701163824448467/Backpropagation-in-Neural-Network-1.webp" alt="Backpropagation-in-Neural-Network-1" style="zoom:67%;" />

<center>A simple illustration of how the backpropagation works by adjustments of weights</center>

<center>é€šè¿‡æƒé‡è°ƒæ•´ï¼Œç®€å•å±•ç¤ºåå‘ä¼ æ’­çš„å·¥ä½œæ–¹å¼</center>



**åå‘ä¼ æ’­çš„é‡è¦æ€§ï¼š**

- **é«˜æ•ˆçš„æƒé‡æ›´æ–°**ï¼šåˆ©ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æŸå¤±å‡½æ•°å¯¹æ¯ä¸ªæƒé‡çš„æ¢¯åº¦ï¼Œä»è€Œé«˜æ•ˆåœ°æ›´æ–°å‚æ•°ã€‚
- **è‰¯å¥½çš„æ‰©å±•æ€§**ï¼šé€‚ç”¨äºå¤šå±‚ç»“æ„å’Œå¤æ‚æ¶æ„ï¼Œæ˜¯æ·±åº¦å­¦ä¹ å¯è¡Œçš„æ ¸å¿ƒç®—æ³•ã€‚
- **è‡ªåŠ¨å­¦ä¹ èƒ½åŠ›**ï¼šè®­ç»ƒè¿‡ç¨‹è‡ªåŠ¨è¿›è¡Œï¼Œæ¨¡å‹ä¼šä¸æ–­è°ƒæ•´è‡ªèº«æ¥ä¼˜åŒ–æ€§èƒ½ã€‚

> **Back Propagation** plays a critical role in how neural networks improve over time. Here's why:
>
> 1. **Efficient Weight Update**: It computes the gradient of the loss function with respect to each weight using the chain rule making it possible to update weights efficiently.
> 2. **Scalability**: The Back Propagation algorithm scales well to networks with multiple layers and complex architectures making deep learning feasible.
> 3. **Automated Learning**: With Back Propagation the learning process becomes automated and the model can adjust itself to optimize its performance.



## åå‘ä¼ æ’­ç®—æ³•çš„å·¥ä½œæµç¨‹

åå‘ä¼ æ’­ç®—æ³•åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ­¥éª¤ï¼š**å‰å‘ä¼ æ’­ï¼ˆForward Passï¼‰** å’Œ **åå‘ä¼ æ’­ï¼ˆBackward Passï¼‰**

### 1. Forward Pass Workå‰å‘ä¼ æ’­

è¾“å…¥æ•°æ®ä»è¾“å…¥å±‚å¼€å§‹ï¼Œç»è¿‡å¸¦æƒé‡çš„è¿æ¥ä¼ é€’åˆ°éšè—å±‚ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªæœ‰ä¸¤ä¸ªéšè—å±‚ h1 å’Œ h2 çš„ç½‘ç»œä¸­ï¼Œh1 çš„è¾“å‡ºä½œä¸º h2 çš„è¾“å…¥ã€‚åœ¨åº”ç”¨æ¿€æ´»å‡½æ•°å‰ï¼Œè¿˜ä¼šåŠ ä¸Šåç½®é¡¹ã€‚

æ¯ä¸€å±‚éƒ½ä¼šè®¡ç®—è¾“å…¥çš„åŠ æƒå’Œï¼ˆè®°ä½œ `a`ï¼‰ï¼Œå†é€šè¿‡å¦‚ ReLU ç­‰æ¿€æ´»å‡½æ•°å¾—åˆ°è¾“å‡º `o`ã€‚æœ€ç»ˆï¼Œè¾“å‡ºå±‚é€šå¸¸ä¼šä½¿ç”¨ softmax æ¿€æ´»å‡½æ•°å°†ç»“æœè½¬æ¢ä¸ºåˆ†ç±»æ¦‚ç‡ã€‚

> ### Working of Back Propagation Algorithm
>
> The Back Propagation algorithm involves two main steps: the **Forward Pass** and the **Backward Pass**.
>
> ### 1. Forward Pass Work
>
> In **forward pass** the input data is fed into the input layer. These inputs combined with their respective weights are passed to hidden layers. For example in a network with two hidden layers (h1 and h2) the output from h1 serves as the input to h2. Before applying an activation function, a bias is added to the weighted inputs.
>
> Each hidden layer computes the weighted sum (`a`) of the inputs then applies an activation function like [**ReLU (Rectified Linear Unit)**](https://www.geeksforgeeks.org/deep-learning/relu-activation-function-in-deep-learning/) to obtain the output (`o`). The output is passed to the next layer where an activation function such as [**softmax**](https://www.geeksforgeeks.org/deep-learning/the-role-of-softmax-in-neural-networks-detailed-explanation-and-applications/) converts the weighted outputs into probabilities for classification.

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/Backpropagation-in-Neural-Network-2.webp" alt="Backpropagation-in-Neural-Network-2" style="zoom:67%;" />

<center>The forward pass using weights and biases</center>

> h1,h2ï¼Œè¡¨ç¤ºéšè—å±‚çš„ä¸¤ä¸ªç¥ç»å…ƒ



### 2. Backward Passåå‘ä¼ æ’­

åå‘ä¼ æ’­é˜¶æ®µä¼šå°†é¢„æµ‹è¾“å‡ºä¸å®é™…è¾“å‡ºçš„è¯¯å·®å‘åä¼ é€’ï¼Œå¹¶è°ƒæ•´æ¯ä¸€å±‚çš„æƒé‡å’Œåç½®ã€‚å¸¸è§çš„è¯¯å·®è®¡ç®—æ–¹æ³•æ˜¯**å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰**ï¼š

$MSE = (\text{Predicted Output} âˆ’ \text{Actual Output})^2$

åœ¨è¯¯å·®è®¡ç®—ä¹‹åï¼Œé€šè¿‡é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦ï¼Œè¿™äº›æ¢¯åº¦ç”¨äºæŒ‡å¯¼æƒé‡å’Œåç½®çš„æ›´æ–°æ–¹å‘å’Œå¹…åº¦ã€‚åå‘ä¼ æ’­è¿‡ç¨‹æ˜¯é€å±‚æ‰§è¡Œçš„ï¼Œ<mark>æ¿€æ´»å‡½æ•°çš„å¯¼æ•°åœ¨æ¢¯åº¦è®¡ç®—ä¸­èµ·ç€å…³é”®ä½œç”¨</mark>ã€‚



**åå‘ä¼ æ’­çš„ç¤ºä¾‹ï¼šæœºå™¨å­¦ä¹ ä¸­çš„æ¡ˆä¾‹**

å‡è®¾æˆ‘ä»¬ä½¿ç”¨ sigmoid æ¿€æ´»å‡½æ•°ï¼Œç›®æ ‡è¾“å‡ºä¸º 0.5ï¼Œå­¦ä¹ ç‡ä¸º 1ã€‚

> ### 2. Backward Pass
>
> In the backward pass the error (the difference between the predicted and actual output) is propagated back through the network to adjust the weights and biases. One common method for error calculation is the [**Mean Squared Error (MSE)**](https://www.geeksforgeeks.org/maths/mean-squared-error/) given by:
>
> $MSE = (\text{Predicted Output} âˆ’ \text{Actual Output})^2$
>
> Once the error is calculated the network adjusts weights using **gradients** which are computed with the chain rule. These gradients indicate how much each weight and bias should be adjusted to minimize the error in the next iteration. The backward pass continues layer by layer ensuring that the network learns and improves its performance. The activation function through its derivative plays a crucial role in computing these gradients during Back Propagation.
>
> 
>
> ## Example of Back Propagation in Machine Learning
>
> Letâ€™s walk through an example of Back Propagation in machine learning. Assume the neurons use the sigmoid activation function for the forward and backward pass. The target output is 0.5 and the learning rate is 1.

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/Backpropagation-in-Neural-Network-3.webp" alt="Backpropagation-in-Neural-Network-3" style="zoom:67%;" />

<center>Example (1) of backpropagation sum</center>



## å‰å‘ä¼ æ’­Forward Propagation

### 1. Initial Calculationåˆå§‹è®¡ç®—

The weighted sum at each node is calculated using:

> $a_j=\sum(w_{i,j}âˆ—x_i)$

Where,

- $a_j$ is the weighted sum of all the inputs and weights at each node
- $w_{i,j}$ represents the weights between the $i^{th}$ input and the $j^{th}$ neuron
- $x_i$ represents the value of the $i^{th}$ input

`O (output):`After applying the activation function to `a`, we get the output of the neuron:

> $o_j = \text{activation function}(a_j)$

### 2. Sigmoid Function

The sigmoid function returns a value between 0 and 1, introducing non-linearity into the model.

> $y_j = \frac{1}{1+e^{âˆ’a_j}}$ 

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/Backpropagation-in-Neural-Network-4.webp" alt="Backpropagation-in-Neural-Network-4" style="zoom:67%;" />

<center>To find the outputs of y3, y4 and y5</center>



### 3. Computing Outputsè¾“å‡ºè®¡ç®—

h1 èŠ‚ç‚¹ï¼š
$$
a_1 = (w_{1,1} \times x_1) + (w_{2,1} \times x_2)
$$
$$
a_1 = (0.2 \times 0.35) + (0.2 \times 0.7) = 0.21
$$

è®¡ç®—å®Œ $a_1$ åï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­è®¡ç®— $y_3$ çš„å€¼ï¼š

$$
y_j = F(a_j) = \frac{1}{1 + e^{-a_1}}
$$
$$
y_3 = F(0.21) = \frac{1}{1 + e^{-0.21}} = 0.56
$$



h2 èŠ‚ç‚¹ï¼š
$$
a_2 = (w_{1,2} \times x_1) + (w_{2,2} \times x_2) = (0.3 \times 0.35) + (0.3 \times 0.7) = 0.315
$$
$$
y_4 = F(0.315) = \frac{1}{1 + e^{-0.315}} = 0.578
$$



è¾“å‡ºèŠ‚ç‚¹ O3ï¼š
$$
a_3 = (w_{1,3} \times y_3) + (w_{2,3} \times y_4) = (0.3 \times 0.56) + (0.9 \times 0.58) = 0.702
$$
$$
y_5 = F(0.702) = \frac{1}{1 + e^{-0.702}} = 0.67
$$



> At h1 node
>
> Once we calculated the a1 value, we can now proceed to find the y3 value:
>
> Similarly find the values of y4 at h2 and y5 at O3



<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/Backpropagation-in-Neural-Network-5.webp" alt="Backpropagation-in-Neural-Network-5" style="zoom:67%;" />

<center>Values of y3, y4 and y5</center>



### 4. Error Calculationè¯¯å·®è®¡ç®—

Our actual output is 0.5 but we obtained 0.67**.** To calculate the error we can use the below formula:

> $Error_j=y_{target}âˆ’y_5$ 

=> 0.5âˆ’0.67=âˆ’0.17

Using this error value we will be backpropagating.



## åå‘ä¼ æ’­Back Propagation

### 1. Calculating Gradientsè®¡ç®—æ¢¯åº¦

The change in each weight is calculated as:

> $Î”w_{ij}=Î·Ã—Î´_jÃ—O_j$

Where:

- $Î´_j$ is the error term for each unit,
- $Î·$ is the learning rate.

### 2. Output Unit Errorè¾“å‡ºå±‚è¯¯å·®

For O3:

> $Î´_5=y_5(1âˆ’y_5)(y_{target}âˆ’y_5)$

=0.67(1âˆ’0.67)(âˆ’0.17)=âˆ’0.0376

### 3. Hidden Unit Erroréšè—å±‚è¯¯å·®

For h1:

> $Î´_3=y_3(1âˆ’y_3)(w_{1,3}Ã—Î´_5)$

=0.56(1âˆ’0.56)(0.3Ã—âˆ’0.0376)=âˆ’0.0027



For h2:

> $Î´_4=y_4(1âˆ’y_4)(w_{2,3}Ã—Î´_5)$

=0.59(1âˆ’0.59)(0.9Ã—âˆ’0.0376)=âˆ’0.0819



### 4. Weight Updatesæƒé‡æ›´æ–°

For the weights from hidden to output layer:

> $Î”w_{2,3}=1Ã—(âˆ’0.0376)Ã—0.59=âˆ’0.022184$

New weight:

> $w_{2,3}(new)=âˆ’0.022184+0.9=0.877816$

For weights from input to hidden layer:

> $Î”w_{1,1}=1Ã—(âˆ’0.0027)Ã—0.35=0.000945$

New weight:

> $w_{1,1}(new)=0.000945+0.2=0.200945$

Similarly other weights are updated:

- $w_{1,2}(new)=0.273225$
- $w_{1,3}(new)=0.086615$
- $w_{2,1}(new)=0.269445$
- $w_{2,2}(new)=0.18534$

The updated weights are illustrated below

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/Backpropagation-in-Neural-Network-5-20251127160556998.webp" alt="Backpropagation-in-Neural-Network-5" style="zoom:67%;" />

<center>Through backward pass the weights are updated</center>

> ä¸Šå›¾æƒé‡æ²¡æœ‰æ›´æ–°ï¼Œä¾‹å¦‚ï¼š$w_{2,2}$åº”è¯¥æ›´æ–°ä¸º0.18534



After updating the weights the forward pass is repeated yielding:

- y3=0.57
- y4=0.56
- y5=0.61

ä»æœªè¾¾åˆ°ç›®æ ‡å€¼ 0.5ï¼Œå› æ­¤ç»§ç»­è¿›è¡Œåå‘ä¼ æ’­ï¼Œç›´åˆ°æ”¶æ•›ã€‚

> Since y5=0.61 is still not the target output the process of calculating the error and backpropagating continues until the desired output is reached.



This process demonstrates how Back Propagation iteratively updates weights by minimizing errors until the network accurately predicts the output.

> $Error=y_{target}âˆ’y_5$

=0.5âˆ’0.61=âˆ’0.11=0.5âˆ’0.61=âˆ’0.11

This process is said to be continued until the actual output is gained by the neural network.



## Back Propagation Implementation in Python for XOR Problem

**Q: XOR é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ**

> XORï¼ˆå¼‚æˆ–ï¼‰æ˜¯ä¸€ä¸ªç»å…¸çš„é€»è¾‘é—®é¢˜ï¼Œå®ƒçš„è¾“å…¥è¾“å‡ºå¦‚ä¸‹ï¼š
>
> | è¾“å…¥ A | è¾“å…¥ B | è¾“å‡º |
> | ------ | ------ | ---- |
> | 0      | 0      | 0    |
> | 0      | 1      | 1    |
> | 1      | 0      | 1    |
> | 1      | 1      | 0    |
>
> è¿™ä¸ªé—®é¢˜**ä¸èƒ½ç”¨ä¸€æ¡ç›´çº¿åˆ†å¼€**ï¼ˆä¸æ˜¯çº¿æ€§å¯åˆ†çš„ï¼‰ï¼Œæ‰€ä»¥å•å±‚æ„ŸçŸ¥æœºæ— æ³•è§£å†³ï¼Œå¿…é¡»ç”¨**è‡³å°‘ä¸€ä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œ**ã€‚



> â€œ**å•å±‚æ„ŸçŸ¥æœº**â€ï¼ˆSingle-Layer Perceptronï¼‰æ˜¯ç¥ç»ç½‘ç»œæœ€åŸå§‹ã€æœ€ç®€å•çš„å½¢å¼ï¼Œç”± Frank Rosenblatt åœ¨ 1957 å¹´æå‡ºã€‚ç†è§£å®ƒï¼Œæœ‰åŠ©äºæ˜ç™½ä¸ºä»€ä¹ˆåƒ **XOR è¿™æ ·çš„é—®é¢˜æ— æ³•è¢«å®ƒè§£å†³**ï¼Œä»è€Œå¼•å‡ºå¤šå±‚ç¥ç»ç½‘ç»œå’Œåå‘ä¼ æ’­çš„å¿…è¦æ€§ã€‚
>
> å•å±‚æ„ŸçŸ¥æœºç»“æ„ï¼š
>
> - **è¾“å…¥å±‚**ï¼šæ¥æ”¶ç‰¹å¾ï¼ˆæ¯”å¦‚ $x_1, x_2$ï¼‰
> - **è¾“å‡ºå±‚**ï¼š**ç›´æ¥è¾“å‡ºç»“æœ**ï¼ˆæ²¡æœ‰éšè—å±‚ï¼ï¼‰
> - æ¯ä¸ªè¾“å…¥æœ‰ä¸€ä¸ªå¯¹åº”çš„æƒé‡ $w_1, w_2$ï¼Œè¿˜æœ‰ä¸€ä¸ªåç½® $b$
>
> **æ•°å­¦è¡¨è¾¾ï¼š**
> $$
> z = w_1 x_1 + w_2 x_2 + b
> \nonumber
> $$
>
> $$
> \text{output} = \begin{cases} 1 & \text{if } z \geq 0 \\ 0 & \text{if } z < 0 \end{cases}
> \nonumber
> $$
>
> > æ³¨æ„ï¼š**æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼ˆæˆ–åªæœ‰é˜¶è·ƒå‡½æ•°ï¼‰**ï¼Œ**æ²¡æœ‰éšè—å±‚**ï¼Œæ‰€ä»¥å«â€œå•å±‚â€ã€‚
> >
> > é˜¶è·ƒå‡½æ•°æ˜¯â€œç¡¬åˆ¤å†³â€ï¼Œé€‚åˆç†è®ºåˆ†æï¼›ä½†å› ä¸ºä¸å¯å¯¼ï¼Œä¸èƒ½ç”¨äºç°ä»£ç¥ç»ç½‘ç»œçš„è®­ç»ƒã€‚
>
> ------
>
> âœ… å•å±‚æ„ŸçŸ¥æœºèƒ½åšä»€ä¹ˆï¼Ÿ
>
> å®ƒåªèƒ½è§£å†³ **çº¿æ€§å¯åˆ†**ï¼ˆlinearly separableï¼‰çš„é—®é¢˜ã€‚
>
> **ä¾‹å­ï¼šAND é—¨**
>
> | xâ‚   | xâ‚‚   | y    |
> | ---- | ---- | ---- |
> | 0    | 0    | 0    |
> | 0    | 1    | 0    |
> | 1    | 0    | 0    |
> | 1    | 1    | 1    |
>
> âœ… å¯ä»¥ç”¨ä¸€æ¡ç›´çº¿åˆ†å¼€ 0 å’Œ 1 â†’ **çº¿æ€§å¯åˆ†** â†’ **å•å±‚æ„ŸçŸ¥æœºå¯ä»¥å­¦ä¼š**
>
> æ¯”å¦‚ï¼š
> å– (w_1 = 1, w_2 = 1, b = -1.5)
> åˆ™ï¼š
>
> - (0+0-1.5 = -1.5 < 0 â†’ 0)
> - (1+1-1.5 = 0.5 â‰¥ 0 â†’ 1)
>
> å®Œç¾ï¼
>
> ------
>
> **âŒ å•å±‚æ„ŸçŸ¥æœºä¸èƒ½åšä»€ä¹ˆï¼Ÿ**
>
> **XOR é—®é¢˜ï¼ˆå¼‚æˆ–ï¼‰ï¼š**
>
> | xâ‚   | xâ‚‚   | y    |
> | ---- | ---- | ---- |
> | 0    | 0    | 0    |
> | 0    | 1    | 1    |
> | 1    | 0    | 1    |
> | 1    | 1    | 0    |
>
> åœ¨äºŒç»´å¹³é¢ä¸Šç”»å‡ºæ¥ï¼š
>
> ```
> (0,1) â— (y=1)        (1,1) â—‹ (y=0)
> 
> (0,0) â—‹ (y=0)        (1,0) â— (y=1)
> ```
>
> ä½ ä¼šå‘ç°ï¼š**æ— æ³•ç”¨ä¸€æ¡ç›´çº¿æŠŠ â— å’Œ â—‹ å®Œå…¨åˆ†å¼€**ï¼
>
> â†’ è¿™å°±æ˜¯ **éçº¿æ€§å¯åˆ†é—®é¢˜**ã€‚
>
> **ç»“è®º**ï¼š 
>
> > **å•å±‚æ„ŸçŸ¥æœºæ— æ³•è§£å†³ XOR é—®é¢˜**ï¼Œå› ä¸ºå®ƒç¼ºä¹éçº¿æ€§è¡¨è¾¾èƒ½åŠ›ã€‚
>
> ------
>
> ** é‚£æ€ä¹ˆåŠï¼Ÿâ€”â€”å¼•å…¥éšè—å±‚ï¼**
>
> 1969 å¹´ï¼ŒMinsky å’Œ Papert åœ¨ã€ŠPerceptronsã€‹ä¸€ä¹¦ä¸­æŒ‡å‡ºäº†è¿™ä¸ªå±€é™ï¼Œå¯¼è‡´ç¥ç»ç½‘ç»œç ”ç©¶ä¸€åº¦åœæ»ã€‚
>
> ç›´åˆ°åæ¥äººä»¬å‘ç°ï¼š
>
> > **åªè¦åŠ ä¸€ä¸ªéšè—å±‚ï¼Œå¹¶ä½¿ç”¨éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ sigmoidã€ReLUï¼‰ï¼Œç¥ç»ç½‘ç»œå°±èƒ½é€¼è¿‘ä»»æ„å‡½æ•°**ï¼ˆä¸‡èƒ½è¿‘ä¼¼å®šç†ï¼‰ã€‚
>
> äºæ˜¯ï¼Œ**å¤šå±‚æ„ŸçŸ¥æœº**ï¼ˆMLPï¼‰ + **åå‘ä¼ æ’­** æˆä¸ºè§£å†³æ–¹æ¡ˆã€‚
>
> ------
>
> **ğŸ”„ å¯¹æ¯”æ€»ç»“**
>
> | ç‰¹æ€§                | å•å±‚æ„ŸçŸ¥æœº         | å¤šå±‚æ„ŸçŸ¥æœºï¼ˆå¸¦åå‘ä¼ æ’­ï¼‰ |
> | ------------------- | ------------------ | ------------------------ |
> | éšè—å±‚              | âŒ æ²¡æœ‰             | âœ… æœ‰ï¼ˆè‡³å°‘1å±‚ï¼‰          |
> | æ¿€æ´»å‡½æ•°            | é˜¶è·ƒå‡½æ•°ï¼ˆä¸å¯å¯¼ï¼‰ | Sigmoid / ReLUï¼ˆå¯å¯¼ï¼‰   |
> | èƒ½å¦è§£å†³ AND/OR/NOT | âœ… å¯ä»¥             | âœ… å¯ä»¥                   |
> | èƒ½å¦è§£å†³ XOR        | âŒ ä¸è¡Œ             | âœ… å¯ä»¥                   |
> | æ˜¯å¦æ”¯æŒåå‘ä¼ æ’­    | âŒ ä¸æ”¯æŒï¼ˆä¸å¯å¯¼ï¼‰ | âœ… æ”¯æŒ                   |
> | å­¦ä¹ èƒ½åŠ›            | ä»…çº¿æ€§åˆ†ç±»         | éçº¿æ€§å»ºæ¨¡               |
>
> ------
>
> ğŸ“Œ å°çŸ¥è¯†
>
> - â€œæ„ŸçŸ¥æœºâ€ï¼ˆPerceptronï¼‰é€šå¸¸ç‰¹æŒ‡**å•å±‚ã€ä½¿ç”¨é˜¶è·ƒæ¿€æ´»ã€ç”¨æ„ŸçŸ¥æœºå­¦ä¹ è§„åˆ™æ›´æ–°æƒé‡**çš„æ¨¡å‹ã€‚
> - è€Œæˆ‘ä»¬ä»Šå¤©è¯´çš„â€œç¥ç»ç½‘ç»œâ€ï¼Œä¸€èˆ¬æŒ‡**å¤šå±‚ã€å¯å¾®æ¿€æ´»ã€ç”¨æ¢¯åº¦ä¸‹é™+åå‘ä¼ æ’­è®­ç»ƒ**çš„æ¨¡å‹ï¼Œä¹Ÿå« **å¤šå±‚æ„ŸçŸ¥æœº**ï¼ˆMLPï¼‰ï¼Œå°½ç®¡åå­—é‡Œæœ‰â€œæ„ŸçŸ¥æœºâ€ï¼Œä½†å·²ç»å®Œå…¨ä¸åŒäº†ã€‚
>
> 



This code demonstrates how Back Propagation is used in a neural network to solve the XOR problem. The neural network consists of:

### 1. Defining Neural Networkå®šä¹‰ç¥ç»ç½‘ç»œç»“æ„

è¾“å…¥å±‚ï¼š2ä¸ªèŠ‚ç‚¹ï¼Œéšè—å±‚ï¼š4ä¸ªç¥ç»å…ƒï¼Œè¾“å‡ºå±‚ï¼š1ä¸ªç¥ç»å…ƒï¼Œæ¿€æ´»å‡½æ•°ï¼šSigmoid

> We define a neural network as Input layer with 2 inputs, Hidden layer with 4 neurons, Output layer with 1 output neuron and use **Sigmoid** function as activation function.

- **self.input_size = input_size**: stores the size of the input layer
- **self.hidden_size = hidden_size:** stores the size of the hidden layer
- **self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)**: initializes weights for input to hidden layer
- **self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)**: initializes weights for hidden to output layer
- **self.bias_hidden = np.zeros((1, self.hidden_size)):** initializes bias for hidden layer
- **self.bias_output = np.zeros((1, self.output_size)):** initializes bias for output layer



```python3

import numpy as np


class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        self.weights_input_hidden = np.random.randn(
            self.input_size, self.hidden_size)
        self.weights_hidden_output = np.random.randn(
            self.hidden_size, self.output_size)

        self.bias_hidden = np.zeros((1, self.hidden_size))
        self.bias_output = np.zeros((1, self.output_size))

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        return x * (1 - x)
```



### 2. Defining Feed Forward Networkå®šä¹‰å‰å‘ä¼ æ’­

In Forward pass inputs are passed through the network activating the hidden and output layers using the sigmoid function.

- **self.hidden_activation = np.dot(X, self.weights_input_hidden) + self.bias_hidden**: calculates activation for hidden layer
- **self.hidden_output= self.sigmoid(self.hidden_activation)**: applies activation function to hidden layer
- **self.output_activation= np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output:** calculates activation for output layer
- **self.predicted_output = self.sigmoid(self.output_activation):** applies activation function to output layer





```python3

def feedforward(self, X):
    self.hidden_activation = np.dot(
        X, self.weights_input_hidden) + self.bias_hidden
    self.hidden_output = self.sigmoid(self.hidden_activation)

    self.output_activation = np.dot(
        self.hidden_output, self.weights_hidden_output) + self.bias_output
    self.predicted_output = self.sigmoid(self.output_activation)

    return self.predicted_output
```



### 3. Defining Backward Networkå®šä¹‰åå‘ä¼ æ’­

In Backward pass or Back Propagation the errors between the predicted and actual outputs are computed. The gradients are calculated using the derivative of the sigmoid function and weights and biases are updated accordingly.

- **output_error = y - self.predicted_output:** calculates the error at the output layer
- **output_delta = output_error * self.sigmoid_derivative(self.predicted_output):** calculates the delta for the output layer
- **hidden_error = np.dot(output_delta, self.weights_hidden_output.T):** calculates the error at the hidden layer
- **hidden_delta = hidden_error \* self.sigmoid_derivative(self.hidden_output):** calculates the delta for the hidden layer
- **self.weights_hidden_output += np.dot(self.hidden_output.T, output_delta) * learning_rate:** updates weights between hidden and output layers
- **self.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate:** updates weights between input and hidden layers



```python3

def backward(self, X, y, learning_rate):
    output_error = y - self.predicted_output
    output_delta = output_error * \
        self.sigmoid_derivative(self.predicted_output)

    hidden_error = np.dot(output_delta, self.weights_hidden_output.T)
    hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)

    self.weights_hidden_output += np.dot(self.hidden_output.T,
                                         output_delta) * learning_rate
    self.bias_output += np.sum(output_delta, axis=0,
                               keepdims=True) * learning_rate
    self.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate
    self.bias_hidden += np.sum(hidden_delta, axis=0,
                               keepdims=True) * learning_rate
```



### 4. Training Networkè®­ç»ƒç½‘ç»œ

The network is trained over 10,000 epochs using the Back Propagation algorithm with a learning rate of 0.1 progressively reducing the error.

- **output = self.feedforward(X):** computes the output for the current inputs
- **self.backward(X, y, learning_rate):** updates weights and biases using Back Propagation
- **loss = np.mean(np.square(y - output)):** calculates the mean squared error (MSE) loss



```python3

def train(self, X, y, epochs, learning_rate):
    for epoch in range(epochs):
        output = self.feedforward(X)
        self.backward(X, y, learning_rate)
        if epoch % 4000 == 0:
            loss = np.mean(np.square(y - output))
            print(f"Epoch {epoch}, Loss:{loss}")
```

### 5. Testing Neural Networkæµ‹è¯•ç¥ç»ç½‘ç»œ

- **X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]):** defines the input data
- **y = np.array([[0], [1], [1], [0]]):** defines the target values
- **nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1):** initializes the neural network
- **nn.train(X, y, epochs=10000, learning_rate=0.1):** trains the network
- **output = nn.feedforward(X):** gets the final predictions after training





```python3

X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)
nn.train(X, y, epochs=10000, learning_rate=0.1)

output = nn.feedforward(X)
print("Predictions after training:")
print(output)
```

**Output:**

![Screenshot-2025-03-07-130223](https://raw.githubusercontent.com/GMyhf/img/main/img/Screenshot-2025-03-07-130223.png)

<center>Trained Model</center>



è®­ç»ƒåˆæœŸæŸå¤±ä¸º 0.2713ï¼Œé€æ­¥ä¸‹é™åˆ° 0.0066ï¼ˆç¬¬8000è½®ï¼‰ã€‚æœ€ç»ˆæ¨¡å‹å¯ä»¥å¾ˆå¥½åœ°é€¼è¿‘ XOR å‡½æ•°çš„è¾“å‡ºï¼Œå³ï¼š

- å¯¹äºè¾“å…¥ [0,0] å’Œ [1,1]ï¼Œè¾“å‡ºæ¥è¿‘ 0

- å¯¹äºè¾“å…¥ [0,1] å’Œ [1,0]ï¼Œè¾“å‡ºæ¥è¿‘ 1

  

> - The output shows the training progress of a neural network over 10,000 epochs. Initially the loss was high (0.2713) but it gradually decreased as the network learned reaching a low value of 0.0066 by epoch 8000.
> - The final predictions are close to the expected XOR outputs: approximately 0 for [0, 0] and [1, 1] and approximately 1 for [0, 1] and [1, 0] indicating that the network successfully learned to approximate the XOR function.



## åå‘ä¼ æ’­çš„ä¼˜ç‚¹

**æ˜“äºå®ç°**ï¼šé€‚åˆåˆå­¦è€…ï¼Œæ— éœ€å¤ªå¤šç¥ç»ç½‘ç»œèƒŒæ™¯

**ç»“æ„ç®€å•ï¼Œçµæ´»åº”ç”¨**ï¼šä»ç®€å•å‰é¦ˆåˆ°å¤æ‚å·ç§¯/å¾ªç¯ç½‘ç»œéƒ½å¯ä½¿ç”¨

**é«˜æ•ˆ**ï¼šç›´æ¥æ ¹æ®è¯¯å·®æ›´æ–°æƒé‡ï¼Œå­¦ä¹ é€Ÿåº¦å¿«

**è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›**ï¼šæœ‰åŠ©äºæ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°æ›´å¥½

**å¯æ‰©å±•æ€§å¥½**ï¼šé€‚ç”¨äºå¤§å‹æ•°æ®é›†å’Œæ·±å±‚æ¨¡å‹

> **Advantages of Back Propagation for Neural Network Training**
>
> The key benefits of using the Back Propagation algorithm are:
>
> 1. **Ease of Implementation:** Back Propagation is beginner-friendly requiring no prior neural network knowledge and simplifies programming by adjusting weights with error derivatives.
> 2. **Simplicity and Flexibility:** Its straightforward design suits a range of tasks from basic feedforward to complex convolutional or recurrent networks.
> 3. **Efficiency**: Back Propagation accelerates learning by directly updating weights based on error especially in deep networks.
> 4. **Generalization:** It helps models generalize well to new data improving prediction accuracy on unseen examples.
> 5. **Scalability:** The algorithm scales efficiently with larger datasets and more complex networks making it ideal for large-scale tasks.



## åå‘ä¼ æ’­é¢ä¸´çš„æŒ‘æˆ˜

**æ¢¯åº¦æ¶ˆå¤±**ï¼šåœ¨æ·±å±‚ç½‘ç»œä¸­æ¢¯åº¦å¯èƒ½è¿‡å°ï¼Œå¯¼è‡´å­¦ä¹ å›°éš¾ï¼ˆç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨ sigmoid/tanh æ—¶ï¼‰

**æ¢¯åº¦çˆ†ç‚¸**ï¼šæ¢¯åº¦å¯èƒ½å˜å¾—è¿‡å¤§ï¼Œä½¿è®­ç»ƒä¸ç¨³å®š

**è¿‡æ‹Ÿåˆ**ï¼šæ¨¡å‹ç»“æ„è¿‡äºå¤æ‚æ—¶ï¼Œå¯èƒ½è®°ä½è®­ç»ƒé›†è€Œéå­¦ä¹ ä¸€èˆ¬æ€§è§„å¾‹

> **Challenges with Back Propagation**
>
> While Back Propagation is useful it does face some challenges:
>
> 1. **Vanishing Gradient Problem**: In deep networks the gradients can become very small during Back Propagation making it difficult for the network to learn. This is common when using activation functions like sigmoid or tanh.
> 2. **Exploding Gradients**: The gradients can also become excessively large causing the network to diverge during training.
> 3. **Overfitting:** If the network is too complex it might memorize the training data instead of learning general patterns.



## å®Œæ•´xor_nnä»£ç 

```python
# å¯¹äºXORé—®é¢˜ï¼ˆè¾“å…¥ä¸º[0,0], [0,1], [1,0], [1,1]ï¼‰ï¼ŒæœŸæœ›è¾“å‡ºä¸º[0,1,1,0]
# æ‰‹åŠ¨å®ç°åå‘ä¼ æ’­ï¼Œæ²¡æœ‰ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè¿™æœ‰åŠ©äºç†è§£åº•å±‚åŸç†
# https://www.geeksforgeeks.org/backpropagation-in-neural-network/
import numpy as np


class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size  # è¾“å…¥ç‰¹å¾ç»´åº¦
        self.hidden_size = hidden_size  # éšè—å±‚ç¥ç»å…ƒæ•°é‡
        self.output_size = output_size  # è¾“å‡ºå±‚ç¥ç»å…ƒæ•°é‡

        # è¾“å…¥å±‚åˆ°éšè—å±‚çš„æƒé‡ï¼Œå½¢çŠ¶ä¸º (è¾“å…¥ç»´åº¦, éšè—å±‚ç»´åº¦)
        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)
        # éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡ï¼Œå½¢çŠ¶ä¸º (éšè—å±‚ç»´åº¦, è¾“å‡ºå±‚ç»´åº¦)
        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)

        # éšè—å±‚çš„åç½®ï¼Œå½¢çŠ¶ä¸º (1, éšè—å±‚ç»´åº¦)
        self.bias_hidden = np.zeros((1, self.hidden_size))
        # è¾“å‡ºå±‚çš„åç½®ï¼Œå½¢çŠ¶ä¸º (1, è¾“å‡ºå±‚ç»´åº¦)
        self.bias_output = np.zeros((1, self.output_size))

    def sigmoid(self, x):  # æ¿€æ´»å‡½æ•°ï¼Œå°†è¾“å…¥å‹ç¼©åˆ°(0,1)åŒºé—´
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        return x * (1 - x)  # Sigmoidçš„å¯¼æ•°ï¼Œç”¨äºåå‘ä¼ æ’­ä¸­çš„æ¢¯åº¦è®¡ç®—

    def feedforward(self, X):
        # éšè—å±‚è®¡ç®—
        self.hidden_activation = np.dot(X, self.weights_input_hidden) + self.bias_hidden  # çº¿æ€§å˜æ¢
        self.hidden_output = self.sigmoid(self.hidden_activation)  # æ¿€æ´»å‡½æ•°

        # è¾“å‡ºå±‚è®¡ç®—
        self.output_activation = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output
        self.predicted_output = self.sigmoid(self.output_activation)

        return self.predicted_output

    def backward(self, X, y, learning_rate):
        # è®¡ç®—è¾“å‡ºå±‚è¯¯å·®
        output_error = y - self.predicted_output  # è¯¯å·® = çœŸå®å€¼ - é¢„æµ‹å€¼
        # è®¡ç®—è¾“å‡ºå±‚çš„deltaï¼ˆæ¢¯åº¦çš„ä¸€éƒ¨åˆ†ï¼ŒæŸå¤±å¯¹æ¿€æ´»è¾“å…¥çš„æ¢¯åº¦ï¼‰
        output_delta = output_error * self.sigmoid_derivative(self.predicted_output)  # Delta = è¯¯å·® Ã— æ¿€æ´»å‡½æ•°å¯¼æ•°
        # output_delta = (y - Å·) * Ïƒ'(z_output)

        # è®¡ç®—éšè—å±‚è¯¯å·®ï¼ˆåå‘ä¼ æ’­ï¼‰
        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)  # å°†è¯¯å·®ä»è¾“å‡ºå±‚åå‘ä¼ æ’­åˆ°éšè—å±‚
        # hidden_error = output_delta @ W_hidden_output^T
        # è®¡ç®—éšè—å±‚çš„deltaï¼ˆæŸå¤±å¯¹éšè—å±‚æ¿€æ´»è¾“å…¥çš„æ¢¯åº¦ï¼‰
        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)  # Delta = è¯¯å·® Ã— æ¿€æ´»å‡½æ•°å¯¼æ•°
        # hidden_delta = (hidden_error) * Ïƒ'(z_hidden)

        # æ›´æ–°æƒé‡å’Œåç½®ï¼ˆä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ï¼‰
        # è®¡ç®—å¹¶æ›´æ–°éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡
        self.weights_hidden_output += np.dot(self.hidden_output.T,
                                             output_delta) * learning_rate  # æƒé‡æ›´æ–°é‡ = å­¦ä¹ ç‡ Ã— (éšè—å±‚è¾“å‡ºè½¬ç½® Ã— è¾“å‡ºå±‚delta)
        # W_hidden_output += learning_rate * (hidden_output^T @ output_delta)

        # æ›´æ–°è¾“å‡ºå±‚åç½®ï¼ŒåŸºäºæ‰€æœ‰æ ·æœ¬çš„è¾“å‡ºå±‚deltaæ²¿åˆ—æ±‚å’Œ
        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate  # åç½®æ›´æ–°é‡ = å­¦ä¹ ç‡ Ã— (æ²¿åˆ—æ±‚å’Œè¾“å‡ºå±‚delta)
        # b_output += learning_rate * sum(output_delta)

        # è®¡ç®—å¹¶æ›´æ–°ä»è¾“å…¥å±‚åˆ°éšè—å±‚çš„æƒé‡çš„æ¢¯åº¦
        self.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate  # æƒé‡æ›´æ–°é‡ = å­¦ä¹ ç‡ Ã— (è¾“å…¥æ•°æ®è½¬ç½® Ã— éšè—å±‚delta)
        # W_input_hidden += learning_rate * (X^T @ hidden_delta)

        # æ›´æ–°éšè—å±‚åç½®ï¼ŒåŸºäºæ‰€æœ‰æ ·æœ¬çš„éšè—å±‚deltaæ²¿åˆ—æ±‚å’Œ
        # axis=0ï¼šæ²¿åˆ—æ±‚å’Œï¼Œèšåˆæ‰€æœ‰æ ·æœ¬çš„æ¢¯åº¦
        # keepdims=Trueï¼šä¿æŒåŸçŸ©é˜µçš„è¡Œæ•°ç»´åº¦ï¼Œç¡®ä¿åç½®æ›´æ–°çš„å½¢çŠ¶å…¼å®¹æ€§
        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate  # åç½®æ›´æ–°é‡ = å­¦ä¹ ç‡ Ã— (æ²¿åˆ—æ±‚å’Œéšè—å±‚delta)
        # b_hidden += learning_rate * sum(hidden_delta)

    def train(self, X, y, epochs, learning_rate):
        for epoch in range(epochs):
            output = self.feedforward(X)  # å‰å‘ä¼ æ’­
            self.backward(X, y, learning_rate)  # åå‘ä¼ æ’­ä¸å‚æ•°æ›´æ–°
            if epoch % 4000 == 0:
                loss = np.mean(np.square(y - output))  # è®¡ç®—å‡æ–¹è¯¯å·®
                print(f"Epoch {epoch}, Loss:{loss}")


X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

# è¾“å…¥ç»´åº¦ 2ï¼ˆäºŒç»´äºŒè¿›åˆ¶ç‰¹å¾ï¼‰ï¼Œéšè—å±‚4ä¸ªç¥ç»å…ƒï¼Œè¾“å‡ºå±‚1ä¸ªç¥ç»å…ƒï¼ˆäºŒåˆ†ç±»é—®é¢˜ï¼‰
nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)
# è®­ç»ƒæ€»è½®æ¬¡, å­¦ä¹ ç‡
nn.train(X, y, epochs=10000, learning_rate=0.1)

output = nn.feedforward(X)
print("Predictions after training:")
print(output)
"""
Epoch 0, Loss:0.2653166263520884
Epoch 4000, Loss:0.007000926683956338
Epoch 8000, Loss:0.001973630232951721
Predictions after training:
[[0.03613239]
 [0.96431351]
 [0.96058291]
 [0.03919372]]
"""
```

