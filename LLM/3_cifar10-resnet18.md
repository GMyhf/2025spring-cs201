# PyTorch å®ä¾‹ - CIFAR-10 å›¾åƒåˆ†ç±»é¡¹ç›®

Updated 1123 GMT+8 Nov 28 2025

2025 summer, Complied by Hongfei Yan



ç”¨åˆ°çš„æ•°æ®é›†æ˜¯ The CIFAR-10 datasetï¼Œhttps://www.cs.toronto.edu/~kriz/cifar.html

> The CIFAR-10 and CIFAR-100 datasets are labeled subsets of the [80 million tiny images](http://people.csail.mit.edu/torralba/tinyimages/) dataset. CIFAR-10 and CIFAR-100 were created by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
>
> ## The CIFAR-10 dataset
>
> The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.
>
> The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.
>
> Here are the classes in the dataset, as well as 10 random images from each:
>
> | airplane   | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane1.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane2.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane3.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane4.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane5.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane6.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane7.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane8.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane9.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane10.png) |
> | ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
> | automobile | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile1.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile2.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile3.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile4.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile5.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile6.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile7.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile8.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile9.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile10.png) |
> | bird       | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird1.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird2.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird3.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird4.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird5.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird6.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird7.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird8.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird9.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird10.png) |
> | cat        | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat1.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat2.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat3.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat4.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat5.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat6.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat7.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat8.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat9.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat10.png) |
> | deer       | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer1.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer2.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer3.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer4.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer5.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer6.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer7.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer8.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer9.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer10.png) |
> | dog        | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog1.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog2.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog3.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog4.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog5.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog6.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog7.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog8.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog9.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog10.png) |
> | frog       | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog1.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog2.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog3.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog4.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog5.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog6.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog7.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog8.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog9.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog10.png) |
> | horse      | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse1.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse2.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse3.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse4.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse5.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse6.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse7.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse8.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse9.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse10.png) |
> | ship       | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship1.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship2.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship3.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship4.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship5.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship6.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship7.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship8.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship9.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship10.png) |
> | truck      | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck1.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck2.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck3.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck4.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck5.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck6.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck7.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck8.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck9.png) | ![img](https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck10.png) |
>
> The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. "Automobile" includes sedans, SUVs, things of that sort. "Truck" includes only big trucks. Neither includes pickup trucks.
>
> ### Download
>
> If you're going to use this dataset, please cite the tech report at the bottom of this page. 
>
> | Version                                                      | Size   | md5sum                           |
> | ------------------------------------------------------------ | ------ | -------------------------------- |
> | [CIFAR-10 python version](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz) | 163 MB | c58f30108f718f92721af3b95e74349a |
> | [CIFAR-10 Matlab version](https://www.cs.toronto.edu/~kriz/cifar-10-matlab.tar.gz) | 175 MB | 70270af85842c9e89bb428ec9976c926 |
> | [CIFAR-10 binary version (suitable for C programs)](https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz) | 162 MB | c32a1d4ab5d03f1284b67883e8d87530 |
>
> ### Baseline results
>
> You can find some baseline replicable results on this dataset [on the project page for cuda-convnet](http://code.google.com/p/cuda-convnet/). These results were obtained with a convolutional neural network. Briefly, they are 18% test error without data augmentation and 11% with. Additionally, [Jasper Snoek](http://www.cs.toronto.edu/~jasper/) has a [new paper](http://hips.seas.harvard.edu/content/practical-bayesian-optimization-machine-learning-algorithms) in which he used Bayesian hyperparameter optimization to find nice settings of the weight decay and other hyperparameters, which allowed him to obtain a test error rate of 15% (without data augmentation) using the architecture of the net that got 18%.



æœ¬æ„æ˜¯è¿è¡Œ https://www.runoob.com/pytorch/pytorch-image-classification.html

ä½†æ˜¯å‡†ç¡®ç‡æ²¡æœ‰è¾¾åˆ°baselineçš„82%ï¼Œæ‰€ä»¥åšäº†æ”¹è¿›ã€‚

## ç”¨ResNet18ç»“æ„ï¼Œé‡æ–°è®­ç»ƒï¼Œåšäº†æ•°æ®å¢å¼º

```python
import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import matplotlib.pyplot as plt
import numpy as np
import time

def main():
    # 1. æ•°æ®å¢å¼º + é¢„å¤„ç†
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(10),  # éšæœºæ—‹è½¬
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # è‰²å½©è°ƒæ•´
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)

    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2, pin_memory=True)

    classes = ('plane', 'car', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck')

    # 2. è®¾ç½®è®¾å¤‡å’Œæ¨¡å‹
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    print("Using device:", device)

    # åŠ è½½é¢„å®šä¹‰çš„ ResNet18 å¹¶ä¿®æ”¹è¾“å‡ºå±‚
    net = models.resnet18(weights=None)
    net.fc = nn.Linear(net.fc.in_features, 10)  # CIFAR10 10 ç±»
    net.to(device)

    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)

    # 3. è®­ç»ƒè¿‡ç¨‹
    best_loss = float('inf')
    patience = 10 # æé«˜è€å¿ƒ
    patience_counter = 0

    start_time = time.time()
    print("Starting training with early stopping...")
    for epoch in range(800):  # å¯é€‚å½“å¢å¤§ epoch
        net.train()
        epoch_loss = 0.0
        for i, data in enumerate(trainloader, 0):
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            if i % 100 == 99:
                print(f"[{epoch + 1}, {i + 1:5d}] loss: {loss.item():.3f}")

        avg_loss = epoch_loss / len(trainloader)
        print(f"[{epoch+1}] Avg Loss: {avg_loss:.3f}")

        if avg_loss < best_loss - 1e-4:
            best_loss = avg_loss
            patience_counter = 0
        else:
            patience_counter += 1
            print(f"No improvement. Patience: {patience_counter}/{patience}")
            if patience_counter >= patience:
                print("Early stopping triggered.")
                break



    end_time = time.time()
    execution_time_minutes = (end_time - start_time) / 60
    print(f"âœ… Training completed in {execution_time_minutes:.2f} minutes.")


    # ä¿å­˜æ¨¡å‹
    torch.save(net.state_dict(), './resnet18_cifar10_data_augument.pth')

    # 4. æµ‹è¯•å‡†ç¡®ç‡
    correct = 0
    total = 0
    net.eval()
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print(f"Accuracy on test images: {100 * correct / total:.2f}%")

    # æ¯ç±»å‡†ç¡®ç‡
    class_correct = list(0. for _ in range(10))
    class_total = list(0. for _ in range(10))
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            c = (predicted == labels).squeeze()
            for i in range(len(labels)):
                class_correct[labels[i]] += c[i].item()
                class_total[labels[i]] += 1

    for i in range(10):
        print(f'Accuracy of {classes[i]:5s}: {100 * class_correct[i] / class_total[i]:.2f}%')

    # --- å¯è§†åŒ–é¢„æµ‹ ---

    def imshow_grid(images, labels, preds=None, classes=None, rows=8, cols=8):
        images = images.cpu() / 2 + 0.5  # unnormalize
        npimg = images.numpy()
        fig, axes = plt.subplots(rows, cols, figsize=(cols * 1.5, rows * 1.5))
        for i in range(rows * cols):
            r, c = divmod(i, cols)
            ax = axes[r, c]
            img = np.transpose(npimg[i], (1, 2, 0))
            ax.imshow(img)
            title = f'{classes[labels[i]]}'
            if preds is not None:
                title += f'\nâ†’ {classes[preds[i]]}'
            ax.set_title(title, fontsize=8)
            ax.axis('off')
        plt.tight_layout()
        plt.show()

    # è·å–ä¸€æ‰¹å›¾åƒç”¨äºæ˜¾ç¤º
    dataiter = iter(testloader)
    images, labels = next(dataiter)
    while images.size(0) < 64:
        more_images, more_labels = next(dataiter)
        images = torch.cat([images, more_images], dim=0)
        labels = torch.cat([labels, more_labels], dim=0)
    images = images[:64]
    labels = labels[:64]

    # é¢„æµ‹
    net.eval()
    with torch.no_grad():
        outputs = net(images.to(device))
        _, predicted = torch.max(outputs, 1)

    # æ˜¾ç¤ºå›¾åƒç½‘æ ¼
    imshow_grid(images, labels, predicted.cpu(), classes=classes, rows=8, cols=8)

if __name__ == "__main__":
    import torch.multiprocessing
    torch.multiprocessing.set_start_method('spawn', force=True)
    main()

```



### åœ¨Mac Studioï¼ˆApple M1 Ultra, 64GBï¼‰è¿è¡Œ

ç»“æœå¦‚ä¸‹ï¼š

```
/Users/hfyan/miniconda3/bin/python /Users/hfyan/Desktop/LLMs-from-scratch-main/runoob/pytorch-image-classification/image_classification-ResNet18-RandomCropFlipLR_Cosine.py 
Using device: mps
Starting training with early stopping...
[1,   100] loss: 1.752
[1,   200] loss: 1.675
[1,   300] loss: 1.654
[1] Avg Loss: 1.806
[2,   100] loss: 1.497
[2,   200] loss: 1.459
[2,   300] loss: 1.453
[2] Avg Loss: 1.520
[3,   100] loss: 1.534
[3,   200] loss: 1.383
[3,   300] loss: 1.167
[3] Avg Loss: 1.372
[4,   100] loss: 1.390
[4,   200] loss: 1.221
[4,   300] loss: 1.238
[4] Avg Loss: 1.244
[5,   100] loss: 1.089
[5,   200] loss: 1.020
[5,   300] loss: 1.133
[5] Avg Loss: 1.159
......
No improvement. Patience: 1/10
[192,   100] loss: 0.187
[192,   200] loss: 0.293
[192,   300] loss: 0.356
[192] Avg Loss: 0.302
[193,   100] loss: 0.223
[193,   200] loss: 0.348
[193,   300] loss: 0.309
[193] Avg Loss: 0.301
[194,   100] loss: 0.303
[194,   200] loss: 0.219
[194,   300] loss: 0.280
[194] Avg Loss: 0.304
No improvement. Patience: 1/10
[195,   100] loss: 0.279
[195,   200] loss: 0.296
[195,   300] loss: 0.313
[195] Avg Loss: 0.296
[196,   100] loss: 0.254
[196,   200] loss: 0.385
[196,   300] loss: 0.280
[196] Avg Loss: 0.300
No improvement. Patience: 1/10
[197,   100] loss: 0.216
[197,   200] loss: 0.298
[197,   300] loss: 0.290
[197] Avg Loss: 0.298
No improvement. Patience: 2/10
[198,   100] loss: 0.267
[198,   200] loss: 0.218
[198,   300] loss: 0.367
[198] Avg Loss: 0.290
[199,   100] loss: 0.270
[199,   200] loss: 0.240
[199,   300] loss: 0.351
[199] Avg Loss: 0.301
No improvement. Patience: 1/10
[200,   100] loss: 0.251
[200,   200] loss: 0.227
[200,   300] loss: 0.302
[200] Avg Loss: 0.299
No improvement. Patience: 2/10
[201,   100] loss: 0.348
[201,   200] loss: 0.301
[201,   300] loss: 0.193
[201] Avg Loss: 0.299
No improvement. Patience: 3/10
[202,   100] loss: 0.313
[202,   200] loss: 0.329
[202,   300] loss: 0.305
[202] Avg Loss: 0.295
No improvement. Patience: 4/10
[203,   100] loss: 0.266
[203,   200] loss: 0.254
[203,   300] loss: 0.307
[203] Avg Loss: 0.294
No improvement. Patience: 5/10
[204,   100] loss: 0.372
[204,   200] loss: 0.295
[204,   300] loss: 0.348
[204] Avg Loss: 0.300
No improvement. Patience: 6/10
[205,   100] loss: 0.392
[205,   200] loss: 0.353
[205,   300] loss: 0.306
[205] Avg Loss: 0.296
No improvement. Patience: 7/10
[206,   100] loss: 0.262
[206,   200] loss: 0.213
[206,   300] loss: 0.396
[206] Avg Loss: 0.293
No improvement. Patience: 8/10
[207,   100] loss: 0.293
[207,   200] loss: 0.204
[207,   300] loss: 0.337
[207] Avg Loss: 0.291
No improvement. Patience: 9/10
[208,   100] loss: 0.413
[208,   200] loss: 0.294
[208,   300] loss: 0.315
[208] Avg Loss: 0.295
No improvement. Patience: 10/10
Early stopping triggered.
âœ… Training completed in 79.91 minutes.
Accuracy on test images: 83.57%
Accuracy of plane: 83.70%
Accuracy of car  : 92.20%
Accuracy of bird : 78.70%
Accuracy of cat  : 60.40%
Accuracy of deer : 79.30%
Accuracy of dog  : 77.40%
Accuracy of frog : 90.30%
Accuracy of horse: 92.50%
Accuracy of ship : 88.50%
Accuracy of truck: 92.70%

Process finished with exit code 0
```



<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/202507280020181.jpg" alt="d561be986280572516ac1023e9ff715c" style="zoom:50%;" />



### åœ¨Clabäº‘è™šæ‹Ÿæœºï¼ˆå†…å­˜32GBï¼Œè™šæ‹ŸCPU32ï¼‰è¿è¡Œ

ç»“æœå¦‚ä¸‹ï¼š

```
/home/rocky/AI_literacy/.venv/bin/python /home/rocky/AI_literacy/CIFAR-10_nn.py 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:15<00:00, 11.1MB/s]
/usr/lib64/python3.9/tarfile.py:2288: RuntimeWarning: The default behavior of tarfile extraction has been changed to disallow common exploits (including CVE-2007-4559). By default, absolute/parent paths are disallowed and some mode bits are cleared. See https://access.redhat.com/articles/7004769 for more details.
  warnings.warn(
Using device: cpu
Starting training with early stopping...
/home/rocky/AI_literacy/.venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
  warnings.warn(warn_msg)
[1,   100] loss: 1.893
[1,   200] loss: 1.658
[1,   300] loss: 1.698
[1] Avg Loss: 1.804
[2,   100] loss: 1.399
[2,   200] loss: 1.580
[2,   300] loss: 1.407
[2] Avg Loss: 1.517
[3,   100] loss: 1.346
[3,   200] loss: 1.492
[3,   300] loss: 1.097
[3] Avg Loss: 1.373
[4,   100] loss: 1.309
[4,   200] loss: 1.182
[4,   300] loss: 1.187
[4] Avg Loss: 1.244
[5,   100] loss: 1.104
[5,   200] loss: 1.084
[5,   300] loss: 1.081
[5] Avg Loss: 1.151
......
[267,   100] loss: 0.278
[267,   200] loss: 0.226
[267,   300] loss: 0.188
[267] Avg Loss: 0.249
[268,   100] loss: 0.218
[268,   200] loss: 0.220
[268,   300] loss: 0.206
[268] Avg Loss: 0.256
No improvement. Patience: 1/10
[269,   100] loss: 0.200
[269,   200] loss: 0.175
[269,   300] loss: 0.298
[269] Avg Loss: 0.256
No improvement. Patience: 2/10
[270,   100] loss: 0.325
[270,   200] loss: 0.265
[270,   300] loss: 0.319
[270] Avg Loss: 0.258
No improvement. Patience: 3/10
[271,   100] loss: 0.316
[271,   200] loss: 0.139
[271,   300] loss: 0.263
[271] Avg Loss: 0.251
No improvement. Patience: 4/10
[272,   100] loss: 0.260
[272,   200] loss: 0.184
[272,   300] loss: 0.141
[272] Avg Loss: 0.256
No improvement. Patience: 5/10
[273,   100] loss: 0.263
[273,   200] loss: 0.327
[273,   300] loss: 0.246
[273] Avg Loss: 0.250
No improvement. Patience: 6/10
[274,   100] loss: 0.193
[274,   200] loss: 0.221
[274,   300] loss: 0.227
[274] Avg Loss: 0.250
No improvement. Patience: 7/10
[275,   100] loss: 0.185
[275,   200] loss: 0.354
[275,   300] loss: 0.254
[275] Avg Loss: 0.258
No improvement. Patience: 8/10
[276,   100] loss: 0.354
[276,   200] loss: 0.326
[276,   300] loss: 0.246
[276] Avg Loss: 0.250
No improvement. Patience: 9/10
[277,   100] loss: 0.183
[277,   200] loss: 0.383
[277,   300] loss: 0.295
[277] Avg Loss: 0.256
No improvement. Patience: 10/10
Early stopping triggered.
âœ… Training completed in 636.54 minutes.
Accuracy on test images: 83.67%
Accuracy of plane: 84.70%
Accuracy of car  : 91.90%
Accuracy of bird : 84.00%
Accuracy of cat  : 65.90%
Accuracy of deer : 81.10%
Accuracy of dog  : 77.60%
Accuracy of frog : 89.40%
Accuracy of horse: 85.00%
Accuracy of ship : 91.70%
Accuracy of truck: 85.40%

Process finished with exit code 0

```



![f44849630ac92c19ff8f0b2e922801ab](https://raw.githubusercontent.com/GMyhf/img/main/img/f44849630ac92c19ff8f0b2e922801ab.png)



<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/b353056ebdc1085718fc43ab6d2a1316.png" alt="b353056ebdc1085718fc43ab6d2a1316" style="zoom: 50%;" />



### Q: ä¹‹å‰ç‰ˆæœ¬ `image_classification-ResNet18.py` æ²¡æœ‰baselineå¥½ï¼Ÿ

> ä½ ç°åœ¨çš„ç»“æœæ˜¯ï¼š
>
> - **Test Accuracy:** 76.77%
> - **Test Error Rate:** 100% - 76.77% = **23.23%**
>
> è€Œä½ å¼•ç”¨çš„ **baselineï¼ˆæ¥è‡ª cuda-convnet é¡¹ç›®ï¼‰**æä¾›çš„æ˜¯ï¼š
>
> | æ–¹æ³•                       | Test Error (%) | Test Accuracy (%) |
> | -------------------------- | -------------- | ----------------- |
> | Baseline CNNï¼ˆæ— æ•°æ®å¢å¼ºï¼‰ | **18.0%**      | **82.0%**         |
> | Bayesian ä¼˜åŒ–åï¼ˆæ— å¢å¼ºï¼‰  | **15.0%**      | **85.0%**         |
> | CNNï¼ˆå¸¦æ•°æ®å¢å¼ºï¼‰          | **11.0%**      | **89.0%**         |
>
> ------
>
> ğŸ†š å¯¹æ¯”åˆ†æï¼š
>
> | æŒ‡æ ‡              | ä½ çš„æ¨¡å‹ï¼ˆResNet18ï¼‰ | baseline æœ€å¥½  | å·®è·                |
> | ----------------- | -------------------- | -------------- | ------------------- |
> | Test Accuracy (%) | **76.77%**           | **89.0%**      | â†“ ~12.2%            |
> | ç½‘ç»œç»“æ„          | ResNet18             | è‡ªå®šä¹‰ CNN     | ResNet18 æ˜¯æ›´å¼ºæ¨¡å‹ |
> | æ•°æ®å¢å¼º          | å¦                   | æ˜¯             | âœ… å¯æå‡            |
> | è¶…å‚è°ƒä¼˜          | é»˜è®¤                 | æœ‰ï¼ˆBayesianï¼‰ | âœ… å¯æå‡            |
>
> ------
>
> ## ğŸ§  ç»“è®ºï¼š
>
> ä½ å½“å‰çš„ç»“æœæ˜¯ **å¾ˆä¸é”™çš„èµ·ç‚¹**ï¼Œè€ƒè™‘åˆ°ï¼š
>
> - ä½¿ç”¨çš„æ˜¯æ ‡å‡† `ResNet18`ï¼Œä¸”æ²¡æœ‰åšä»»ä½• **æ•°æ®å¢å¼º**ï¼›
> - æ²¡æœ‰ç‰¹åˆ«è°ƒ `lr`ã€`weight_decay`ã€`batch_size` ç­‰è¶…å‚æ•°ï¼›
> - æ²¡æœ‰ç”¨é¢å¤– trickï¼Œå¦‚ CosineLRã€Mixupã€Label Smoothing ç­‰ï¼›
> - ç”¨çš„æ˜¯ **MPS åç«¯**ï¼ˆé€Ÿåº¦å¿«ï¼Œä½†å¯èƒ½ç¨³å®šæ€§å’Œ cuda è¿˜å·®ç‚¹ï¼‰ï¼›
>
> **å› æ­¤ï¼Œä½ çš„ç»“æœåŸºæœ¬å’Œâ€œæœªè°ƒä¼˜çš„ ResNet18 åŸºçº¿â€æŒå¹³ã€‚**
>
> ------
>
> âœ… å¦‚ä½•è¿½å¹³ç”šè‡³è¶…è¶Š baselineï¼Ÿ
>
> ä½ çš„ ResNet18 å¾ˆå¼ºï¼Œä½†ä½ å¯ä»¥ç»§ç»­æå‡å‡†ç¡®ç‡ï¼š
>
> | æ–¹æ³•                    | è¯´æ˜                                        |
> | ----------------------- | ------------------------------------------- |
> | âœ… åŠ æ•°æ®å¢å¼º            | `transforms.RandomCrop + HorizontalFlip`    |
> | âœ… è°ƒä¼˜å­¦ä¹ ç‡/è°ƒåº¦å™¨     | ä½¿ç”¨ `CosineAnnealingLR`ã€warmup ç­‰         |
> | âœ… å¼•å…¥æ­£åˆ™åŒ–æŠ€å·§        | åŠ  weight_decayï¼Œdropout æˆ– label smoothing |
> | âœ… æ¢æ›´é«˜æ•ˆæ¨¡å‹ï¼ˆå¯é€‰ï¼‰  | MobileNetV2 æˆ– EfficientNet                 |
> | âœ… åŠ å…¥éªŒè¯é›† early stop | é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæå‡æ³›åŒ–                        |
> | âœ… å¤šè½®è®­ç»ƒ+å†»ç»“å¾®è°ƒ     | å¾®è°ƒ ImageNet æƒé‡éƒ¨åˆ†                      |
>
> ------
>
> æ˜¯å¦éœ€è¦æˆ‘å¸®ä½ **åŠ æ•°æ®å¢å¼ºç‰ˆæœ¬**ï¼Ÿå‡ è¡Œä»£ç å°±å¯ä»¥åœ¨ `transform` ä¸­æ·»åŠ ï¼Œè®© ResNet18 å‘æŒ¥æ›´å¤§æ½œåŠ›ã€‚
>
> ```python
> transform_train = transforms.Compose([
>     transforms.RandomCrop(32, padding=4),
>     transforms.RandomHorizontalFlip(),
>     transforms.ToTensor(),
>     transforms.Normalize(mean, std),
> ])
> ```
>
> æ˜¯å¦éœ€è¦æˆ‘ç›´æ¥ç»™å‡ºå¯¹æ¯”å®Œæ•´è®­ç»ƒè„šæœ¬ï¼Ÿå¯ä»¥ä¸€é”®æ›¿æ¢ä½ å½“å‰ç‰ˆæœ¬ã€‚



### Q: transforms.Resize(224), # ResNet è¾“å…¥è¦æ±‚è‡³å°‘224x224ï¼Œç°åœ¨ä¸éœ€è¦äº†ï¼Ÿ

> ä½ é—®å¾—éå¸¸å¥½ï¼Œè¿™æ¶‰åŠåˆ° **ResNet18 ä¸ CIFAR-10 è¾“å…¥å›¾åƒå°ºå¯¸çš„å…¼å®¹æ€§é—®é¢˜**ï¼š
>
> ------
>
> ğŸ” ResNet18 çš„åŸå§‹è®¾è®¡è¾“å…¥å°ºå¯¸
>
> - åŸå§‹ ImageNet é¢„è®­ç»ƒçš„ ResNet18 è¾“å…¥å°ºå¯¸æ˜¯ **224Ã—224**ã€‚
>
> - å› æ­¤å¸¸è§ç”¨æ³•ä¸­é€šå¸¸åŒ…å«ï¼š
>
>   ```python
>   transforms.Resize(256),
>   transforms.CenterCrop(224),
>   ```
>
> ------
>
> âœ… åœ¨ CIFAR-10 ä¸Šä½¿ç”¨ ResNet18 ä¸éœ€è¦ `Resize(224)` çš„æƒ…å†µ
>
> å› ä¸ºä½ åœ¨è®­ç»ƒä¸­ä½¿ç”¨çš„æ˜¯ï¼š
>
> ```python
> models.resnet18(weights=None)
> ```
>
> è¿™è¡¨ç¤ºï¼š
>
> - **æ²¡æœ‰åŠ è½½é¢„è®­ç»ƒæƒé‡**
> - ç½‘ç»œä»å¤´å¼€å§‹è®­ç»ƒ
> - æ‰€ä»¥ä½ å¯ä»¥**è‡ªç”±åœ°æ›¿æ¢ç¬¬ä¸€å±‚å·ç§¯**æˆ–ç›´æ¥ç”¨é»˜è®¤çš„ï¼Œ**ä¸éœ€è¦æ”¾å¤§åˆ° 224Ã—224**
>
> è€Œä¸”ä½ çš„æ•°æ®ï¼ˆCIFAR-10ï¼‰æ˜¯ `32x32`ï¼Œæ‰€ä»¥ torchvision ä¸­å¸¸è§çš„å¤„ç†æ–¹å¼æ˜¯ï¼š
>
> - **ä¿ç•™åŸå§‹å°ºå¯¸**
> - åªä½¿ç”¨ï¼š`RandomCrop(32, padding=4)` + `RandomHorizontalFlip()` å°±è¶³å¤Ÿäº†
>
> ------
>
> ğŸ§  å¦‚æœä½ æƒ³ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
>
> å¦‚æœä½ æ‰“ç®—ä½¿ç”¨ï¼š
>
> ```python
> models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
> ```
>
> åˆ™ä½ å¿…é¡»ï¼š
>
> 1. **è°ƒæ•´è¾“å…¥å›¾åƒå¤§å°åˆ° â‰¥224x224**ï¼š
>
>    ```python
>    transforms.Resize(256),
>    transforms.CenterCrop(224),
>    ```
>
> 2. æˆ–è€…ä¿®æ”¹æ¨¡å‹ç¬¬ä¸€å±‚ï¼š
>
>    å°†ï¼š
>
>    ```python
>    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
>    model.maxpool = nn.Identity()
>    ```
>
> ------
>
> âœ… ç»“è®º
>
> | ç”¨æ³•                                           | æ˜¯å¦éœ€è¦ Resize(224) | è¯´æ˜                |
> | ---------------------------------------------- | -------------------- | ------------------- |
> | `weights=None`ï¼ˆä»å¤´è®­ç»ƒï¼‰                     | âŒ ä¸éœ€è¦             | å¯ç›´æ¥ç”¨ 32Ã—32 å›¾åƒ |
> | `weights=ResNet18_Weights.DEFAULT`ï¼ˆç”¨é¢„è®­ç»ƒï¼‰ | âœ… éœ€è¦               | éœ€Resizeåˆ°224x224   |
>
> ------
>
> æ˜¯å¦éœ€è¦æˆ‘å¸®ä½ å†™ä¸€ä»½ä½¿ç”¨ **é¢„è®­ç»ƒResNet18** + è‡ªåŠ¨è°ƒæ•´å°ºå¯¸çš„å®Œæ•´ä»£ç ç‰ˆæœ¬ï¼Ÿ



### Q:  **ç²¾åº¦ (Accuracy)** ä»ç„¶æ¯” **baseline (82%)** ç¨ä½ï¼Œå¯èƒ½çš„åŸå› å’Œæ”¹è¿›å»ºè®®ï¼š

> ä½ çš„è®­ç»ƒç»“æœå·²ç»ç›¸å½“ä¸é”™ï¼Œä½† **ç²¾åº¦ (Accuracy)** ä»ç„¶æ¯” **baseline (82%)** ç¨ä½ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½çš„åŸå› å’Œæ”¹è¿›å»ºè®®ï¼š
>
> ------
>
> âœ… å¯èƒ½çš„åŸå› ï¼š
>
> 1. **æ²¡æœ‰æ•°æ®å¢å¼º**ï¼š
>    - ä½ çš„å½“å‰æ•°æ®å¢å¼ºè®¾ç½®ä¸­ä»…åŒ…å«äº† `RandomCrop` å’Œ `RandomHorizontalFlip`ã€‚è¿™å¯¹ ResNet18 æ¥è¯´å¯èƒ½ä¸å¤Ÿï¼Œå°¤å…¶æ˜¯åœ¨ CIFAR-10 è¿™ç§å°è§„æ¨¡æ•°æ®é›†ä¸Šã€‚Baseline çš„ 82% å‡†ç¡®ç‡æ˜¯ä½¿ç”¨äº† **æ›´å¤šçš„æ•°æ®å¢å¼º**ã€‚
> 2. **å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰è®¾ç½®é—®é¢˜**ï¼š
>    - ä½ çš„å­¦ä¹ ç‡è®¾ç½®æ˜¯ `lr=0.1`ï¼Œè¿™å¯èƒ½ç¨å¾®æœ‰äº›é«˜ï¼Œå¯¼è‡´ç½‘ç»œåœ¨è®­ç»ƒåˆæœŸå°±å‘ç”Ÿäº†éœ‡è¡ï¼Œæ— æ³•æœ‰æ•ˆæ”¶æ•›ã€‚
>    - ä½ å¯ä»¥å°è¯• **é€æ­¥é™ä½å­¦ä¹ ç‡**ï¼Œæ¯”å¦‚ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆ`lr_scheduler`ï¼‰æˆ–è€… **æ›´ä½çš„åˆå§‹å­¦ä¹ ç‡**ï¼ˆå¦‚ 0.01 æˆ– 0.001ï¼‰ã€‚
> 3. **è®­ç»ƒè½®æ•°ï¼ˆEpochsï¼‰ä¸è¶³**ï¼š
>    - è™½ç„¶ä½ è®¾ç½®äº† **100 ä¸ª epoch**ï¼Œä½†æ ¹æ®è®­ç»ƒè¿‡ç¨‹æ¥çœ‹ï¼Œæ¨¡å‹åœ¨ 15 åˆ†é’Ÿå†…åªè®­ç»ƒäº†å¤§çº¦ **24 ä¸ª epoch**ã€‚è¿™æ„å‘³ç€å¯èƒ½è¿˜éœ€è¦æ›´å¤šçš„è®­ç»ƒæ—¶é—´æ¥è¿›ä¸€æ­¥é™ä½æŸå¤±ã€‚
> 4. **æ—©æœŸåœæ­¢ï¼ˆEarly Stoppingï¼‰**ï¼š
>    - æ—©æœŸåœæ­¢å¯èƒ½åœ¨æŸå¤±å°šæœªå®Œå…¨é™ä½æ—¶å°±åœæ­¢äº†è®­ç»ƒã€‚ä½ å¯èƒ½ä¼šå¸Œæœ› **æ”¾å®½æ—©æœŸåœæ­¢çš„æ¡ä»¶**ï¼Œè®©è®­ç»ƒç»§ç»­æ›´å¤š epochï¼Œç›´åˆ°ç²¾åº¦è¾¾åˆ°æ›´é«˜æ°´å¹³ã€‚
>
> ------
>
> âœ… æ”¹è¿›å»ºè®®ï¼š
>
> 1. **å¢åŠ æ•°æ®å¢å¼º**ï¼š
>
> - ä¸ºäº†æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå»ºè®®æ·»åŠ æ›´å¤šçš„æ•°æ®å¢å¼ºæ“ä½œã€‚å¯ä»¥åŠ å…¥ï¼š
>   - `RandomRotation(10)`ï¼šéšæœºæ—‹è½¬å›¾åƒ
>   - `ColorJitter`ï¼šè°ƒæ•´å›¾åƒçš„äº®åº¦ã€å¯¹æ¯”åº¦ç­‰
>   - `RandomAffine`ï¼šéšæœºä»¿å°„å˜æ¢
>
> ä¿®æ”¹åçš„æ•°æ®å¢å¼ºä»£ç ï¼š
>
> ```python
> transform_train = transforms.Compose([
>     transforms.RandomCrop(32, padding=4),
>     transforms.RandomHorizontalFlip(),
>     transforms.RandomRotation(10),  # éšæœºæ—‹è½¬
>     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # è‰²å½©è°ƒæ•´
>     transforms.ToTensor(),
>     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
> ])
> ```
>
> 2. **è°ƒæ•´å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰**ï¼š
>
> - **é€æ­¥é™ä½å­¦ä¹ ç‡**ï¼šå»ºè®®åœ¨è®­ç»ƒæ—¶ **ä½¿ç”¨åŠ¨æ€å­¦ä¹ ç‡**ï¼Œå¯ä»¥å°è¯•æ›´ä½çš„åˆå§‹å­¦ä¹ ç‡ï¼ˆå¦‚ 0.01ï¼‰ï¼Œå¹¶è®¾ç½®ä¸€ä¸ªå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆ`lr_scheduler`ï¼‰æ¥é€æ­¥é™ä½å­¦ä¹ ç‡ã€‚
>
> ```python
> optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)
> scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)  # æ¯20ä¸ªepochå°†å­¦ä¹ ç‡é™ä½50%
> ```
>
> 3. **æ”¾å®½ Early Stopping çš„æ¡ä»¶**ï¼š
>
> - æé«˜ `patience`ï¼ˆä¾‹å¦‚ä» 5 æ”¹ä¸º 10ï¼‰ï¼Œæˆ–è€…å°†åœæ­¢æ¡ä»¶è®¾ä¸ºæ›´å®½æ¾çš„æ ‡å‡†ï¼Œæ¯”å¦‚ **å¢åŠ éªŒè¯é›†å‡†ç¡®ç‡çš„æŒç»­æå‡**ã€‚
>
> ```python
> patience = 10  # æé«˜è€å¿ƒ
> ```
>
> 4. **å¢åŠ è®­ç»ƒè½®æ•°ï¼ˆEpochsï¼‰**ï¼š
>
> - å°è¯•å¢åŠ è®­ç»ƒè½®æ•°ï¼ˆä¾‹å¦‚ 200-300ï¼‰ï¼Œå³ä½¿ä½¿ç”¨æ—©æœŸåœæ­¢ï¼Œå¢åŠ è®­ç»ƒæ—¶é—´ä»ç„¶æœ‰åŠ©äºæ¨¡å‹çš„æ”¶æ•›ã€‚
>
> 5. **æ£€æŸ¥æ•°æ®é›†çš„åˆ’åˆ†**ï¼š
>
> - ç¡®ä¿ä½ åœ¨è®­ç»ƒå’ŒéªŒè¯é›†ä¸Šæ²¡æœ‰æ•°æ®æ³„æ¼ï¼Œå¹¶ä¸”ç¡®ä¿ **æ•°æ®åŠ è½½æ—¶æ²¡æœ‰éšæœºæ€§** å½±å“æ¨¡å‹è¯„ä¼°ï¼ˆæ¯”å¦‚ä½¿ç”¨å›ºå®šçš„éšæœºç§å­ï¼‰ã€‚
>
> 6. **ä½¿ç”¨æ•°æ®å¢å¼ºï¼ˆå¦‚ MixUp æˆ– Cutoutï¼‰**ï¼š
>
> - ä½ å¯ä»¥è¿›ä¸€æ­¥å°è¯• **MixUp**ï¼ˆé€šè¿‡çº¿æ€§æ’å€¼è¿›è¡Œæ•°æ®å¢å¼ºï¼‰æˆ– **Cutout**ï¼ˆéšæœºé®æŒ¡ä¸€éƒ¨åˆ†å›¾åƒï¼‰ç­‰æ–¹æ³•ï¼Œå·²çŸ¥åœ¨ CIFAR-10 ä¸Šè¿™äº›æ–¹æ³•æœ‰åŠ©äºæé«˜å‡†ç¡®ç‡ã€‚
>
> ------
>
> âœ… å°ç»“ï¼š
>
> ä½ çš„æ¨¡å‹è®­ç»ƒå·²ç»éå¸¸æ¥è¿‘ baseline çš„ç»“æœï¼Œç»è¿‡æ•°æ®å¢å¼ºã€é€‚å½“çš„å­¦ä¹ ç‡è°ƒèŠ‚ã€æ›´å¤šè®­ç»ƒè½®æ¬¡ç­‰ä¼˜åŒ–åï¼Œåº”è¯¥èƒ½è¿›ä¸€æ­¥æå‡å‡†ç¡®ç‡ã€‚ä½ è¿˜å¯ä»¥å°è¯• **ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹**ï¼Œæˆ–è€… **è¿ç§»å­¦ä¹ **ï¼Œè¿™äº›æ–¹æ³•èƒ½å¤Ÿè®©æ¨¡å‹æ›´å¿«æ”¶æ•›ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾ƒå°æ•°æ®é›†ä¸Šã€‚
>
> ------
>
> æ˜¯å¦éœ€è¦æˆ‘è¿›ä¸€æ­¥å¸®åŠ©ä½ é›†æˆ **å­¦ä¹ ç‡è°ƒåº¦å™¨**ã€**æ›´å¤šæ•°æ®å¢å¼º**ã€**éªŒè¯é›†è¯„ä¼°**ç­‰åŠŸèƒ½ï¼Œæˆ–è€…ç›´æ¥ç»™å‡ºä¸€ä¸ªå…¨æ–°çš„ç‰ˆæœ¬è¿›è¡Œæ”¹è¿›ï¼Ÿ



> ğŸ”§ æ–°æ”¹è¿›ç‚¹ï¼š
>
> 1. **æ•°æ®å¢å¼º**ï¼š
>    - å¢åŠ äº† `RandomRotation` å’Œ `ColorJitter`ï¼Œå¯ä»¥è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
> 2. **å­¦ä¹ ç‡è°ƒåº¦å™¨**ï¼š
>    - ä½¿ç”¨ `CosineAnnealingLR` æ¥åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ï¼Œå¸®åŠ©æ¨¡å‹åœ¨è®­ç»ƒåæœŸæ›´å¹³æ»‘åœ°æ”¶æ•›ã€‚
> 3. **æ—©æœŸåœæ­¢**ï¼š
>    - è°ƒæ•´äº† `patience` ä¸º 10 è½®ï¼Œå…è®¸æ¨¡å‹æœ‰æ›´å¤šæ—¶é—´æ¥æ”¶æ•›ï¼Œé˜²æ­¢è¿‡æ—©åœæ­¢ã€‚
> 4. **è®­ç»ƒæ—¶é—´ç»Ÿè®¡**ï¼š
>    - è¾“å‡ºäº†æ€»è®­ç»ƒæ—¶é—´ï¼Œæ–¹ä¾¿äº†è§£è®­ç»ƒæ¶ˆè€—ã€‚
> 5. **æ¨¡å‹ä¿å­˜**ï¼š
>    - ä¿å­˜äº†è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œæ–¹ä¾¿ä»¥ååŠ è½½å’Œæ¨ç†ã€‚
>
> ------
>
> ä¸‹ä¸€æ­¥ï¼š
>
> ä½ å¯ä»¥è¿è¡Œè¿™ä¸ªè„šæœ¬ï¼Œçœ‹æ˜¯å¦èƒ½ **è¶…è¶Š baseline çš„ 82%** å‡†ç¡®ç‡ã€‚å¦‚æœç»“æœä¸ç†æƒ³ï¼Œå¯ä»¥è¿›ä¸€æ­¥è°ƒæ•´è¶…å‚æ•°ã€æ·»åŠ æ›´å¤šçš„æŠ€å·§ï¼ˆå¦‚ MixUpã€Cutoutï¼‰ã€‚



### **Q: è§£è¯»ä»£ç **

```
è§£è¯»ï¼š    # æ¯ç±»å‡†ç¡®ç‡
    class_correct = list(0. for _ in range(10))
    class_total = list(0. for _ in range(10))
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            c = (predicted == labels).squeeze()
            for i in range(len(labels)):
                class_correct[labels[i]] += c[i].item()
                class_total[labels[i]] += 1
```

> è¿™æ®µä»£ç ç”¨äºè®¡ç®— **æ¯ä¸€ç±»çš„å‡†ç¡®ç‡**ï¼Œå…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š
>
> ------
>
> ğŸ§© ä»£ç è§£è¯»
>
> ```python
> # åˆ›å»ºä¸¤ä¸ªåˆ—è¡¨ï¼Œåˆ†åˆ«ç”¨äºå­˜å‚¨æ¯ä¸€ç±»çš„æ­£ç¡®é¢„æµ‹æ•°å’Œæ€»æ ·æœ¬æ•°
> class_correct = list(0. for _ in range(10))  # å­˜å‚¨æ¯ä¸ªç±»åˆ«æ­£ç¡®é¢„æµ‹çš„æ¬¡æ•°
> class_total = list(0. for _ in range(10))    # å­˜å‚¨æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
> ```
>
> - `class_correct[i]` ç”¨äºè®°å½•ç±»åˆ« `i` çš„æ­£ç¡®é¢„æµ‹æ¬¡æ•°ã€‚
> - `class_total[i]` ç”¨äºè®°å½•ç±»åˆ« `i` çš„æ€»æ ·æœ¬æ•°ã€‚
>
> `10` æ˜¯ CIFAR-10 æ•°æ®é›†ä¸­çš„ç±»åˆ«æ•°é‡ï¼ˆ10 ç±»ï¼‰ã€‚
>
> ------
>
> ```python
> # ä¸è®¡ç®—æ¢¯åº¦ï¼ŒåŠ é€Ÿæ¨ç†
> with torch.no_grad():
>     for data in testloader:
>         images, labels = data
>         images, labels = images.to(device), labels.to(device)
>         outputs = net(images)
>         _, predicted = torch.max(outputs.data, 1)
> ```
>
> - ä½¿ç”¨ `torch.no_grad()` æ¥ **å…³é—­æ¢¯åº¦è®¡ç®—**ï¼Œè¿™æ ·åœ¨æ¨ç†æ—¶èŠ‚çœå†…å­˜å’Œè®¡ç®—ã€‚
> - `testloader` æ˜¯åŠ è½½æµ‹è¯•é›†çš„æ•°æ®è¿­ä»£å™¨ï¼Œ`images` æ˜¯è¾“å…¥å›¾åƒï¼Œ`labels` æ˜¯å¯¹åº”çš„æ ‡ç­¾ã€‚
> - `outputs = net(images)`ï¼šç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ `net` å¯¹å›¾åƒè¿›è¡Œæ¨ç†ï¼Œå¾—åˆ°æ¯ä¸ªç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡ã€‚
> - `_, predicted = torch.max(outputs.data, 1)`ï¼šé€šè¿‡ `torch.max` è·å– **æ¯å¼ å›¾åƒçš„æœ€å¤§æ¦‚ç‡ç±»åˆ«**ï¼ˆå³æ¨¡å‹çš„é¢„æµ‹æ ‡ç­¾ï¼‰ã€‚
>
> ------
>
> ```python
> # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ­£ç¡®é¢„æµ‹æ•°é‡å’Œæ€»æ ·æœ¬æ•°é‡
> c = (predicted == labels).squeeze()
> for i in range(len(labels)):
>     class_correct[labels[i]] += c[i].item()  # è®°å½•æ­£ç¡®é¢„æµ‹çš„æ•°é‡
>     class_total[labels[i]] += 1              # è®°å½•æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡
> ```
>
> - `(predicted == labels)`ï¼šæ£€æŸ¥é¢„æµ‹æ ‡ç­¾ä¸å®é™…æ ‡ç­¾æ˜¯å¦ä¸€è‡´ï¼Œè¿”å›å¸ƒå°”å€¼ï¼ˆTrue æˆ– Falseï¼‰ã€‚`True` ä»£è¡¨é¢„æµ‹æ­£ç¡®ï¼Œ`False` ä»£è¡¨é¢„æµ‹é”™è¯¯ã€‚
> - `c[i].item()`ï¼šè½¬æ¢å¸ƒå°”å€¼ä¸º 1 æˆ– 0ï¼Œè¡¨ç¤ºè¯¥æ ·æœ¬æ˜¯å¦è¢«æ­£ç¡®åˆ†ç±»ã€‚
> - `class_correct[labels[i]] += c[i].item()`ï¼šå°†æ¯ä¸ªæ­£ç¡®é¢„æµ‹çš„æ ·æœ¬æ•°é‡ç´¯åŠ åˆ°å¯¹åº”ç±»åˆ«çš„ `class_correct` ä¸­ã€‚
> - `class_total[labels[i]] += 1`ï¼šæ¯é‡åˆ°ä¸€ä¸ªæ ·æœ¬ï¼Œå°±ç´¯åŠ åˆ°è¯¥ç±»åˆ«çš„æ€»æ ·æœ¬æ•° `class_total`ã€‚
>
> ------
>
> ğŸ§  æ€»ç»“ï¼š
>
> è¿™æ®µä»£ç çš„ç›®çš„æ˜¯é€šè¿‡éå†æµ‹è¯•é›†ï¼Œè®¡ç®—æ¯ä¸ªç±»åˆ«çš„**æ­£ç¡®é¢„æµ‹æ•°**å’Œ**æ€»æ ·æœ¬æ•°**ã€‚æœ€åï¼Œä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¡ç®— **æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡**ï¼š
>
> ```python
> for i in range(10):
>     accuracy = 100 * class_correct[i] / class_total[i]
>     print(f'Accuracy of {classes[i]:5s}: {accuracy:.2f}%')
> ```
>
> `classes[i]` æ˜¯ CIFAR-10 ä¸­çš„ç±»åˆ«åç§°ï¼ˆå¦‚ `plane`, `car` ç­‰ï¼‰ã€‚
>
> ------
>
> ğŸ”„ å…³é”®ç‚¹ï¼š
>
> 1. **`torch.max(outputs.data, 1)`**: è·å–æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç±»åˆ«ã€‚
> 2. **`class_correct[labels[i]] += c[i].item()`**: ç´¯åŠ æ­£ç¡®é¢„æµ‹çš„æ•°é‡ã€‚
> 3. **`class_total[labels[i]] += 1`**: ç´¯åŠ æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡ã€‚
>
> é€šè¿‡è¿™ä¸ªè¿‡ç¨‹ï¼Œä½ èƒ½å¾—åˆ°æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡ï¼Œæœ‰åŠ©äºäº†è§£æ¨¡å‹åœ¨å“ªäº›ç±»åˆ«ä¸Šè¡¨ç°è¾ƒå¥½æˆ–è¾ƒå·®ã€‚



### **Q: å…·ä½“ç¤ºä¾‹å‘¢ï¼Ÿæ˜¯ä¸€ä¸ªä¸€ä¸ªå›¾åƒæ£€æµ‹å§**

> æ˜¯çš„ï¼Œ**è¿™æ®µä»£ç ç¡®å®æ˜¯åœ¨ä¸€ä¸ªä¸ªå›¾åƒä¸Šæ£€æµ‹é¢„æµ‹æ˜¯å¦æ­£ç¡®**ï¼Œå¹¶æŒ‰ç±»åˆ«ç»Ÿè®¡æ¯ç±»çš„é¢„æµ‹å‡†ç¡®ç‡ã€‚ä¸‹é¢æˆ‘ä»¬ç»“åˆä¸€ä¸ª**å…·ä½“ç¤ºä¾‹**æ¥æ¼”ç¤ºè¿™æ®µä»£ç åœ¨å®é™…è¿è¡Œä¸­åšäº†ä»€ä¹ˆã€‚
>
> ------
>
> ğŸ¯ å‡è®¾åœºæ™¯
>
> æˆ‘ä»¬å‡è®¾æµ‹è¯•é›†ä¸­æŸä¸ª batchï¼ˆå°æ‰¹é‡ï¼‰é‡Œæœ‰ 5 å¼ å›¾åƒï¼ŒçœŸå®æ ‡ç­¾ä¸ºï¼š
>
> ```python
> labels = [1, 3, 3, 5, 1]  # å®é™…ç±»åˆ«ç¼–å·
> ```
>
> æ¨¡å‹é¢„æµ‹ç»“æœä¸ºï¼š
>
> ```python
> predicted = [1, 2, 3, 5, 0]  # æ¨¡å‹é¢„æµ‹çš„ç±»åˆ«ç¼–å·
> ```
>
> é‚£ä¹ˆå¯¹æ¯”ï¼š
>
> | å›¾ç‰‡ç¼–å· | å®é™…æ ‡ç­¾ï¼ˆlabels[i]ï¼‰ | æ¨¡å‹é¢„æµ‹ï¼ˆpredicted[i]ï¼‰ | æ˜¯å¦æ­£ç¡® (==) |
> | -------- | --------------------- | ------------------------ | ------------- |
> | 0        | 1                     | 1                        | âœ… True        |
> | 1        | 3                     | 2                        | âŒ False       |
> | 2        | 3                     | 3                        | âœ… True        |
> | 3        | 5                     | 5                        | âœ… True        |
> | 4        | 1                     | 0                        | âŒ False       |
>
> ç”Ÿæˆå¸ƒå°”æ¯”è¾ƒï¼š
>
> ```python
> c = [True, False, True, True, False]
> ```
>
> ------
>
> ğŸ“Š ç´¯åŠ åˆ° `class_correct` å’Œ `class_total`
>
> åˆå§‹åŒ–ï¼š
>
> ```python
> class_correct = [0.0] * 10
> class_total = [0.0] * 10
> ```
>
> éå†æ¯ä¸ªæ ·æœ¬ï¼š
>
> ```python
> for i in range(5):
>     class_correct[labels[i]] += c[i].item()
>     class_total[labels[i]] += 1
> ```
>
> æ¯ä¸€æ­¥çš„å˜åŒ–å¦‚ä¸‹ï¼š
>
> | i    | labels[i] | c[i]  | class_correct         | class_total           |
> | ---- | --------- | ----- | --------------------- | --------------------- |
> | 0    | 1         | True  | [0, 1, 0, 0, 0â€¦]      | [0, 1, 0, 0, 0â€¦]      |
> | 1    | 3         | False | [0, 1, 0, 0, 0â€¦]      | [0, 1, 0, 1, 0â€¦]      |
> | 2    | 3         | True  | [0, 1, 0, 1, 0â€¦]      | [0, 1, 0, 2, 0â€¦]      |
> | 3    | 5         | True  | [0, 1, 0, 1, 0, 1, â€¦] | [0, 1, 0, 2, 0, 1, â€¦] |
> | 4    | 1         | False | [0, 1, 0, 1, 0, 1, â€¦] | [0, 2, 0, 2, 0, 1, â€¦] |
>
> ------
>
> âœ… å‡†ç¡®ç‡ç»“æœï¼ˆæœ€ç»ˆï¼‰
>
> ```python
> Accuracy of class 1: 1 / 2 = 50.00%
> Accuracy of class 3: 1 / 2 = 50.00%
> Accuracy of class 5: 1 / 1 = 100.00%
> ```
>
> å…¶ä»–ç±»åˆ«æ²¡æœ‰å‡ºç°ï¼Œå‡†ç¡®ç‡ä¸º 0%ã€‚
>
> ------
>
> ğŸ” åœ¨çœŸå®ä»£ç ä¸­ï¼š
>
> è¿™æ®µé€»è¾‘åœ¨ **æ•´ä¸ª testloaderï¼ˆæµ‹è¯•é›†ï¼‰** ä¸Šè¿è¡Œï¼Œæ¯æ¬¡å¤„ç†ä¸€ä¸ª batchï¼ˆå¦‚ 100 å¼ å›¾åƒï¼‰ï¼Œæœ€åç»Ÿè®¡å‡ºå®Œæ•´çš„ per-class accuracyã€‚
>
> ------
>
> âœ… æ€»ç»“ï¼š
>
> - æ˜¯ä¸€ä¸ªå›¾åƒä¸€ä¸ªå›¾åƒåœ¨æ¯”å¯¹ `é¢„æµ‹æ˜¯å¦æ­£ç¡®`ã€‚
> - ç´¯åŠ æ¯ä¸ªç±»åˆ«çš„æ­£ç¡®é¢„æµ‹æ¬¡æ•°å’Œæ ·æœ¬æ€»æ•°ã€‚
> - æœ€åè®¡ç®— `æ¯ç±»å‡†ç¡®ç‡ = æ­£ç¡®é¢„æµ‹æ•° / æ ·æœ¬æ€»æ•°`ã€‚
>
> ä½ å¯ä»¥åœ¨è®­ç»ƒå®Œåéå¸¸ç›´è§‚åœ°çœ‹åˆ°ï¼š**æ¨¡å‹åœ¨å“ªäº›ç±»åˆ«ä¸Šè¡¨ç°å¥½ï¼ˆå‡†ç¡®ç‡é«˜ï¼‰ï¼Œå“ªäº›ç±»åˆ«ä¸å¥½ï¼ˆæ¯”å¦‚çŒ«é€šå¸¸è¾ƒéš¾åˆ†ç±»ï¼‰**ã€‚è¿™å¯¹åˆ†ææ¨¡å‹ç“¶é¢ˆå¾ˆæœ‰å¸®åŠ©ã€‚



### **Q: _, predicted = torch.max(outputs.data, 1) ï¼Ÿ**

> è¿™è¡Œä»£ç ï¼š
>
> ```python
> _, predicted = torch.max(outputs.data, 1)
> ```
>
> æ˜¯ PyTorch ä¸­çš„å¸¸è§å†™æ³•ï¼Œç”¨äºä»æ¨¡å‹çš„è¾“å‡ºä¸­**è·å–æ¯å¼ å›¾ç‰‡é¢„æµ‹çš„ç±»åˆ«ç¼–å·**ï¼ˆä¹Ÿå°±æ˜¯æœ€å¤§å¾—åˆ†å¯¹åº”çš„ä¸‹æ ‡ï¼‰ã€‚ä¸‹é¢æ˜¯è¯¦ç»†è§£è¯»ã€‚
>
> ------
>
> ğŸ“˜ èƒŒæ™¯ï¼šæ¨¡å‹è¾“å‡ºæ˜¯ä»€ä¹ˆï¼Ÿ
>
> åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæ¯”å¦‚ CIFAR-10ï¼Œæ¨¡å‹å¯¹æ¯å¼ å›¾åƒçš„è¾“å‡ºæ˜¯ä¸€ä¸ª **é•¿åº¦ä¸º10çš„å‘é‡**ï¼Œè¡¨ç¤ºæ¯ä¸ªç±»åˆ«çš„å¾—åˆ†ï¼ˆlogitsï¼‰ã€‚ä¾‹å¦‚å¯¹ä¸€å¼ å›¾ç‰‡ï¼Œè¾“å‡ºå¯èƒ½æ˜¯ï¼š
>
> ```python
> [2.1, 1.5, -0.3, 4.2, 0.8, 3.3, 1.1, 0.0, -1.2, 2.0]
> ```
>
> è¿™è¡¨ç¤ºå¯¹ç±»åˆ« 0ï½9 çš„â€œç½®ä¿¡åº¦â€æˆ–â€œåŒ¹é…ç¨‹åº¦â€ï¼Œè¶Šå¤§è¶Šå¯èƒ½æ˜¯æ­£ç¡®ç±»åˆ«ã€‚
>
> ------
>
> ğŸ§  `torch.max(outputs.data, 1)` è§£é‡Š
>
> ```python
> torch.max(tensor, dim)
> ```
>
> è¿™ä¸ªå‡½æ•°ä¼šåœ¨æŒ‡å®šçš„ç»´åº¦ `dim` ä¸Šå¯»æ‰¾æœ€å¤§å€¼ï¼Œå¹¶è¿”å›ä¸¤ä¸ªç»“æœï¼š
>
> 1. **æœ€å¤§å€¼çš„æ•°å€¼**
> 2. **æœ€å¤§å€¼çš„ä¸‹æ ‡ï¼ˆç´¢å¼•ï¼‰**
>
> ä¾‹å¦‚ï¼š
>
> ```python
> outputs.data = tensor([
>   [2.1, 1.5, -0.3, 4.2, 0.8, 3.3, 1.1, 0.0, -1.2, 2.0],
>   [0.5, 2.8, 1.0, 0.7, 0.1, 0.0, 3.9, 0.2, 0.3, 0.5]
> ])
> ```
>
> è¡¨ç¤º 2 å¼ å›¾ç‰‡çš„åˆ†ç±»è¾“å‡ºã€‚
>
> æ‰§è¡Œï¼š
>
> ```python
> _, predicted = torch.max(outputs.data, 1)
> ```
>
> å°±ä¼šå¾—åˆ°ï¼š
>
> ```python
> predicted = tensor([3, 6])
> ```
>
> è¡¨ç¤ºç¬¬ 1 å¼ å›¾åƒé¢„æµ‹ä¸ºç±»åˆ« 3ï¼Œç¬¬ 2 å¼ ä¸ºç±»åˆ« 6ã€‚
>
> ------
>
> ğŸ“Œ ä¸ºä»€ä¹ˆç”¨ `_` æ¥æ”¶ç¬¬ä¸€ä¸ªè¿”å›å€¼ï¼Ÿ
>
> å› ä¸ºä½ åªå…³å¿ƒâ€œå“ªä¸ªç±»åˆ«æ˜¯æœ€å¤§â€ï¼Œä¸å…³å¿ƒâ€œæœ€å¤§å¾—åˆ†æ˜¯å¤šå°‘â€ï¼Œæ‰€ä»¥ç¬¬ä¸€ä¸ªè¿”å›å€¼ï¼ˆå¾—åˆ†ï¼‰å°±ç”¨ `_` å¿½ç•¥æ‰äº†ã€‚
>
> ------
>
> âœ… æ€»ç»“
>
> ```python
> _, predicted = torch.max(outputs.data, 1)
> ```
>
> å«ä¹‰æ˜¯ï¼š
>
> > åœ¨æ¯å¼ å›¾åƒçš„è¾“å‡ºå‘é‡ä¸­ï¼Œæ‰¾å‡ºæœ€å¤§å€¼æ‰€åœ¨çš„ä¸‹æ ‡ `predicted`ï¼Œä½œä¸ºè¯¥å›¾åƒçš„é¢„æµ‹ç±»åˆ«ç¼–å·ã€‚
>
> è¿™ä¸ª `predicted` å°±å¯ä»¥å’Œ `labels` åšæ¯”è¾ƒæ¥åˆ¤æ–­æ˜¯å¦é¢„æµ‹æ­£ç¡®ã€‚





## é™„å½•

### ImageNet å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„ Top-5 é”™è¯¯ç‡

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/image-20250726164756658.png" alt="image-20250726164756658" style="zoom: 67%;" />



**xè½´ï¼šæ¨¡å‹å’Œå¹´ä»½**

ä»å·¦åˆ°å³åˆ—å‡ºäº†ä¸åŒå¹´ä»½çš„ä»£è¡¨æ€§æ¨¡å‹ï¼š

| å¹´ä»½ | æ¨¡å‹         | æ‰€å±æœºæ„/ä½œè€…            |
| ---- | ------------ | ------------------------ |
| 2011 | XRCE         | éæ·±åº¦å­¦ä¹ æ–¹æ³•           |
| 2012 | AlexNet      | Alex Krizhevsky ç­‰äºº     |
| 2013 | ZFNet        | Zeiler & Fergus          |
| 2014 | VGG          | Visual Geometry Group    |
| 2014 | GoogLeNet    | Googleï¼ˆInceptionï¼‰      |
| 2015 | ResNet       | Microsoft Research       |
| 2016 | GoogLeNet-v4 | æ›´æ·±æ›´å¤æ‚çš„ GoogLeNet   |
| ---  | **Human**    | äººç±»å¹³å‡è¡¨ç°ï¼ˆå¯¹æ¯”åŸºçº¿ï¼‰ |

yè½´ï¼š**Top-5 é”™è¯¯ç‡ï¼ˆ%ï¼‰**

è¡¨ç¤ºæ¨¡å‹åœ¨ ImageNet ä¸Šé”™è¯¯é¢„æµ‹çš„æ¦‚ç‡ï¼ˆTop-5ï¼‰ã€‚é”™è¯¯ç‡è¶Šä½ï¼Œæ¨¡å‹æ€§èƒ½è¶Šå¥½ã€‚

ä»å›¾ä¸­å¯ä»¥çœ‹åˆ°ï¼š

- **2011 XRCEï¼šTop-5é”™è¯¯ç‡ä¸º 26.0%**
- **2012 AlexNet** å°†é”™è¯¯ç‡é™ä½åˆ°äº† **16.4%**ï¼Œå¼€å¯æ·±åº¦å­¦ä¹ æ—¶ä»£
- **2015 ResNet** å°†é”™è¯¯ç‡é™è‡³ **3.6%**ï¼Œé¦–æ¬¡è¶…è¿‡äº† **äººç±»**ï¼ˆäººç±»è¯¯å·®ä¸º 5.0%ï¼‰
- **2016 GoogLeNet-v4** é™åˆ°ä»… **3.1%**

------

ğŸ§  ä»€ä¹ˆæ˜¯ Top-5 é”™è¯¯ç‡ï¼Ÿ

åœ¨ ImageNet å›¾åƒåˆ†ç±»ä¸­ï¼Œ**æ¯å¼ å›¾åƒçš„æ ‡ç­¾å±äº 1000 ä¸ªç±»åˆ«ä¹‹ä¸€**ã€‚

- **Top-1 é”™è¯¯ç‡**ï¼šé¢„æµ‹ç¬¬ä¸€åæ˜¯å¦æ­£ç¡®ã€‚
- **Top-5 é”™è¯¯ç‡**ï¼šåªè¦æ¨¡å‹é¢„æµ‹å‡ºçš„å‰ 5 ä¸ªç±»åˆ«ä¸­åŒ…å«æ­£ç¡®ç­”æ¡ˆï¼Œå°±ç®— **é¢„æµ‹æˆåŠŸ**ã€‚

ğŸ‘‰ ä¾‹å¦‚ï¼š
å¦‚æœæ¨¡å‹å¯¹æŸå¼ å›¾ç‰‡çš„è¾“å‡ºæ˜¯ï¼š

```text
1. dog
2. cat
3. horse
4. frog
5. deer
```

è€ŒçœŸå®ç±»åˆ«æ˜¯ **frog**ï¼Œåˆ™ï¼š

- **Top-1** æ˜¯é”™çš„ï¼ˆç¬¬ä¸€åæ˜¯ dogï¼‰
- **Top-5** æ˜¯å¯¹çš„ï¼ˆfrog åœ¨å‰äº”åä¸­ï¼‰

------

âœ… æ€»ç»“

è¿™å¼ å›¾å±•ç¤ºäº†æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ ImageNet ä¸Šçš„é£è·ƒå¼è¿›å±•ï¼Œå°¤å…¶æ˜¯ï¼š

- AlexNetï¼ˆ2012ï¼‰é¦–æ¬¡ä½¿ç”¨æ·±åº¦ CNNï¼Œå¤§å¹…æ”¹è¿›ç»“æœï¼›
- ResNetï¼ˆ2015ï¼‰æå‡ºæ®‹å·®ç½‘ç»œï¼Œé¦–æ¬¡è¶…è¿‡äººç±»ï¼›
- GoogLeNet å’Œ VGG ä¹Ÿéƒ½æ˜¯ä»£è¡¨æ€§çš„ç»å…¸ CNN æ¶æ„ã€‚

**Top-5 é”™è¯¯ç‡ä½ï¼Œæ„å‘³ç€æ¨¡å‹æ›´ç¨³å®šã€é²æ£’æ€§æ›´å¼º**ã€‚åœ¨å·¥ä¸šç•Œã€æ¯”èµ›ä¸­ç»å¸¸ä½œä¸ºé‡è¦è¯„ä»·æŒ‡æ ‡ã€‚

å¦‚æœä½ æƒ³ï¼Œæˆ‘å¯ä»¥è¿›ä¸€æ­¥è§£é‡Šæ¯ä¸ªæ¨¡å‹æ¶æ„çš„æ”¹è¿›è¦ç‚¹ã€‚
